<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Chengcen</title>
    <description></description>
    <link>samshaq19912009.github.io/</link>
    <atom:link href="samshaq19912009.github.io/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Thu, 11 Aug 2016 11:01:46 +0800</pubDate>
    <lastBuildDate>Thu, 11 Aug 2016 11:01:46 +0800</lastBuildDate>
    <generator>Jekyll v3.1.6</generator>
    
      <item>
        <title>Optimal Binary Search Tree</title>
        <description>&lt;h3 id=&quot;problem-descirption&quot;&gt;Problem Descirption&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Recall&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;For a given set of keys, there are lots of valid search Tree&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://upload.wikimedia.org/wikipedia/commons/d/da/Binary_search_tree.svg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Question&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;What is the “best” search tree&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;A good answer&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;A balanced search tree&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Input&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;Frequencies $p1, p2, p_n$ for items $1,2,….n$&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Goal&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;Compute a valid search tree that minimized the weighted average search time:&lt;/p&gt;

&lt;p&gt;$C(T) = \sum_{items i}p_i * [\mbox{search time in T}]$&lt;/p&gt;

&lt;h4 id=&quot;comparison-with-huffman-codes&quot;&gt;Comparison with Huffman Codes&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;Similarities:&lt;/strong&gt;:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Output = a binary tree&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Goal is (essentially) to minimize average depth with respect to
given probabilities&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Differences&lt;/strong&gt;:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;With Huffman codes, constraint was prefix-freeness&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Constraint = search tree property&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;greedy-doesnt-work&quot;&gt;Greedy Doesn’t Work&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Issue&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;With the top-down approach, the choice of root has hard-to-predict repercussions further down the tree.&lt;/p&gt;

&lt;h3 id=&quot;optimal-substructure&quot;&gt;Optimal Substructure&lt;/h3&gt;

&lt;p&gt;Suppose an optimal BST for keys {1, 2, . . . , n} has root r, left subtree T1, right subtree T2.&lt;/p&gt;

&lt;p&gt;T1 is optimal for the keys {1,2,…,r −1} and T2 for the keys {r +1,r +2,…,n}&lt;/p&gt;

&lt;h4 id=&quot;proof-of--optimal-substructure&quot;&gt;Proof of  Optimal Substructure&lt;/h4&gt;

&lt;p&gt;$C(T) = C(T_1) + C(T_2)$&lt;/p&gt;

&lt;h3 id=&quot;the-recurrence&quot;&gt;The Recurrence&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Notation:&lt;/strong&gt; For 1 ≤ i ≤ j ≤ n, let $C_{ij}$ = weighted search cost of an optimal BST for the items {i, i + 1, . . . , j − 1, j} [with probabilities pi,pi+1,…,pj]&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Recurrence:&lt;/strong&gt; 
$C_{i j} = min_{r=i,..j}{ \sum_{k=i}^j p_k + C_{i,r-1} + C_{r+1, j} }$&lt;/p&gt;

&lt;h3 id=&quot;running-time&quot;&gt;Running Time&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;$O(n^2)$ subproblems&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;$O(j-i)$ time to compute subproblem&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;$O(n^3)$ time overall&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;python-implementation&quot;&gt;Python implementation&lt;/h3&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def optimal_BST(key, freq):
    n = len(key)
    cost = [[0 for i in range(n)] for i in range(n)]
    for i in range(n):
        cost[i][i] = freq[i]
    for s in range(1, n):#s means i-j
        for i in range(n-s):
            j = i+s
            
            cost[i][j] = 10000
            for r in range(i,j+1):
                left = cost[i][r-1] if r &amp;gt; i else 0
                right = cost[r+1][j] if r &amp;lt; j else 0
                temp = left + right + sum_freq(freq,i,j)
                if temp &amp;lt; cost[i][j]:
                    cost[i][j] = temp
    return cost[0][n-1]
    
    
def sum_freq(freq, i, j):
    total = sum(freq[i:j+1])
    
    return total


&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

</description>
        <pubDate>Tue, 09 Aug 2016 00:00:00 +0800</pubDate>
        <link>samshaq19912009.github.io/blog/2016/08/09/Optimal_BST/</link>
        <guid isPermaLink="true">samshaq19912009.github.io/blog/2016/08/09/Optimal_BST/</guid>
        
        <category>Algorithms</category>
        
        <category>Notes</category>
        
        <category>Dynamics programming</category>
        
        
        <category>blog</category>
        
      </item>
    
      <item>
        <title>The Bellman-Ford Algorithm</title>
        <description>&lt;p&gt;&lt;strong&gt;Input:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Directed graph G = (V,E), edge lengths $c_e$ for each e ∈ E, source vertex s ∈ V . [Can assume no parallel edges.]&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Goal:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;For every destination v ∈ V , compute the length (sum of edge costs) of a shortest s-v path.&lt;/p&gt;

&lt;h3 id=&quot;on-dijkstras-algorithm&quot;&gt;On Dijkstra’s Algorithm&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Good News&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;O(E log V) running time using heaps&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Bad News&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Not always correct with negative edge lengths&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Not very distributed&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;optimal-substructure&quot;&gt;Optimal Substructure&lt;/h3&gt;

&lt;p&gt;Suppose the input graph G has no negative cycles, &lt;strong&gt;For every v, there is a shortest s-v path with ≤ n − 1 edges&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Key idea:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Artificially restrict the &lt;strong&gt;number of edges&lt;/strong&gt; in a path. Subproblem size ⇐⇒ Number of permitted edges&lt;/p&gt;

&lt;h3 id=&quot;the-recurrence&quot;&gt;The Recurrence&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Notation&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;$L_{i,v}$ minimum length of a s-v path, with no bigger than i edges&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Recurrence&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;$L_{i,v}$ = min ( $L_{i-1,v}$, $min_{u,v \in E} L_{i-1,w} + C_{wv}  $ )&lt;/p&gt;

&lt;h3 id=&quot;running-time&quot;&gt;Running Time&lt;/h3&gt;

&lt;p&gt;$O(n^2)$ Space Complexity&lt;/p&gt;

&lt;p&gt;Outer loop: V loop&lt;/p&gt;

&lt;p&gt;Inner Choice: $\sum_{v \in V} \mbox(indeg) V$&lt;/p&gt;

&lt;p&gt;Time $O(E * V)$&lt;/p&gt;

&lt;h3 id=&quot;python-implentation&quot;&gt;Python Implentation&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;def bellman_ford(graph,	node):
    n =	len(graph.keys())
    distance = [float(&quot;inf&quot;) for _ in range(n)]
    predecessor	[ None for _ in	range(n)]
    distance[node] = 0
    #relax
    for	i in range(1,n-1):
	for u in graph.keys():
            for	v,w in graph[v]:
		if distance[u]+w&amp;lt;distance[v]:
                    distance[v] = distance[u]+w
                    predecessor[v] = u
    #check if there is any negative circle
    for	u in graph.keys():
	for v, w in graph[u]:
            if distance[v] &amp;gt; distance[u] + w:
		print &quot;Found a negative circle!&quot;
		return
    return distance, predecessor


&lt;/code&gt;&lt;/pre&gt;

</description>
        <pubDate>Tue, 09 Aug 2016 00:00:00 +0800</pubDate>
        <link>samshaq19912009.github.io/blog/2016/08/09/Bellman_Ford/</link>
        <guid isPermaLink="true">samshaq19912009.github.io/blog/2016/08/09/Bellman_Ford/</guid>
        
        <category>Algorithms</category>
        
        <category>Notes</category>
        
        <category>Dynamics programming</category>
        
        
        <category>blog</category>
        
      </item>
    
      <item>
        <title>The All-pairs short Paths Problem</title>
        <description>&lt;h3 id=&quot;problem-definition&quot;&gt;Problem Definition&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Input:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Directed graph G = (V , E ) with edge costs ce for each edge e ∈ E .&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Goal:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Compute the length of a shortest u → v path for all pairs of
vertices u, v ∈ V&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Correctly report that G contains a negative cycle.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;N&lt;/strong&gt; invocations of a single-source shortest-path subroutine are needed to solve the all-pairs shortest path problem?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Running time (nonnegative edge costs):&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;V· &lt;strong&gt;Dijkstra&lt;/strong&gt; = O(V E logV)&lt;/p&gt;

&lt;p&gt;$O(n^3 \log{n}) \mbox{if  m }= O(n^2)$&lt;/p&gt;

&lt;p&gt;$O(n^2 \log{n}) \mbox{if  m = O(n)}$&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Running time (general edge costs):&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;V &lt;strong&gt;Bellman-Ford&lt;/strong&gt; = $O(V^2 E)$&lt;/p&gt;

&lt;p&gt;$O(n^3 ) \mbox{if  m }= O(n)$&lt;/p&gt;

&lt;p&gt;$O(n^4) \mbox{if  m }= O(n^2)$&lt;/p&gt;

&lt;h3 id=&quot;floyd-warshall-algorithm&quot;&gt;Floyd-Warshall algorithm&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Time Complexity&lt;/strong&gt; $O(n^3)$&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;At least as good as n Bellman-Fords, better in dense graphs.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In graphs with nonnegative edge costs, competitive with n Dijkstra’s in dense graphs.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;optimal-sub-problem&quot;&gt;Optimal sub-problem&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Optimal substructure lemma:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Suppose G has no negative cost cycle. Let P be a shortest (cycle-free) i-j path with all internal nodes in V (k). Then:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Case 1&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;If k not internal to P, then P is a shortest (cycle-free) i-j path with all internal vertices in V(k−1).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Case 2&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;P1 = shortest (cycle-free) i-k path with all internal nodes in
V(k−1)&lt;/p&gt;

&lt;p&gt;P2 = shortest (cycle-free) k-j path with all internal nodes in
V(k−1&lt;/p&gt;

&lt;h3 id=&quot;code-implementation&quot;&gt;Code Implementation&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;def floyd_warshall_2D(graph):
    n = len(graph.keys())
    A = [[0 for _ in range(n)] for _ in range(n)]
    for i in graph.keys():
        for j in graph.keys():
            if i == j:
                A[i][i] = 0
            if j in graph[i]:
                A[i][j] = graph[i][j]
            else:
                A[i][j] = float(&quot;inf&quot;)
            if i in graph[j]:
                A[j][i] = graph[j][i]
            else:
                A[j][i] = float(&quot;inf&quot;)
    for k in range(n):
        for i in range(n):
            for j in range(n):
                if A[i][j] &amp;gt; A[i][k] + A[k][j]:
                    A[i][j] = A[i][k] + A[j][j]
    return A

&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;johnsons-algorithm-intuition&quot;&gt;Johnson’s algorithm Intuition&lt;/h3&gt;

&lt;p&gt;1 invocation of Bellman-Ford  O(V E logV)&lt;/p&gt;

&lt;p&gt;n n invocations of Dijkstra $O(V E)$&lt;/p&gt;

&lt;h4 id=&quot;reweighting-technique&quot;&gt;Reweighting Technique&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;Lemma 1:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;When all s-t paths in G have the same number of edges, adding a constant M to every edge &lt;strong&gt;does not change&lt;/strong&gt; the shorted s-t path&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Lemma 2&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Set up&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;G = (V , E ) is a directed graph with general edge lengths $c_e$. Fix a real number $p_v$ for each vertex v ∈ V&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Definition:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;For every edge e=(u,v) of G, $c_e′$ := $c_e$ + $p_u$ − $p_v$&lt;/p&gt;

&lt;p&gt;If s-t path has originial length L: the new length is $L + p_s - p_t$&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Summary:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Reweighting using vertex weights $p_v$ adds the same amount (namely, $p_s$ − $p_t$ ) to every s -t path&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Consequence&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Reweighting always leaves the shortest path unchanged.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;After re-weighting, all edegs become positive!!&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;johnsons-algorithm-implementation&quot;&gt;Johnson’s algorithm Implementation&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Input:&lt;/strong&gt; Directed graph G = (V , E ), general edge lengths $c_e$&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Form G′ by adding a new vertex s and a new edge (s,v) with
length 0 for each v ∈ G&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Run Bellman-Ford on G′ with source vertex s. [If B-F detects a negative-cost cycle in G′ (which must lie in G), halt + report this.]&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For each v∈G,define pv =length of a shortest s→v path in G′. For each edgee = (u,v)∈G, define $c_e′$ := $c_e$ + $p_u$ − $p_v$&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For each vertex u of G: Run Dijkstra’s algorithm in G, with edge lengths {ce′}, with source vertex u, to compute the shortest-path distance d′(u,v) for each v ∈ G.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For each pair u, v ∈ G , return the shortest-path distance d(u,v):=d′(u,v)−pu +pvp&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

</description>
        <pubDate>Tue, 09 Aug 2016 00:00:00 +0800</pubDate>
        <link>samshaq19912009.github.io/blog/2016/08/09/All_pair_shorted_path/</link>
        <guid isPermaLink="true">samshaq19912009.github.io/blog/2016/08/09/All_pair_shorted_path/</guid>
        
        <category>Algorithms</category>
        
        <category>Notes</category>
        
        <category>Dynamics programming</category>
        
        
        <category>blog</category>
        
      </item>
    
      <item>
        <title>Binary Tree Level Order Traversal</title>
        <description>&lt;p&gt;&lt;a href=&quot;https://leetcode.com/problems/binary-tree-level-order-traversal/&quot;&gt;Leetcode Link&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;solution-1--using-queue&quot;&gt;Solution 1 : Using Queue&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;class Solution(object):
    def levelOrder(self, root):
        &quot;&quot;&quot;
        :type root: TreeNode
        :rtype: List[List[int]]
        &quot;&quot;&quot;
        if not root:
            return []
        q = [root]
        ans = []
        while q:
            ans.append([node.val for node in q])
            new_q = []
            for node in q:
                if node.left:
                    new_q.append(node.left)
                if node.right:
                    new_q.append(node.right)
            q = new_q
        return ans

&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;solution-2-depth-first-search&quot;&gt;Solution 2: Depth First Search&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;class Solution(object):
    def levelOrder(self, root):
        &quot;&quot;&quot;
        :type root: TreeNode
        :rtype: List[List[int]]
        &quot;&quot;&quot;
        if not root:
            return []
        ans = []
        self.bfs(root,0, ans)
        return ans
        
    def bfs(self, node, level, ans):
        if not node:
            return
        
        if len(ans) &amp;lt; level+1:
            ans.append([])
        ans[level].append(node.val)
        self.bfs(node.left, level+1, ans)
        self.bfs(node.right, level+1, ans)


&lt;/code&gt;&lt;/pre&gt;
</description>
        <pubDate>Tue, 09 Aug 2016 00:00:00 +0800</pubDate>
        <link>samshaq19912009.github.io/blog/2016/08/09/Course_Schedule_leetcode/</link>
        <guid isPermaLink="true">samshaq19912009.github.io/blog/2016/08/09/Course_Schedule_leetcode/</guid>
        
        <category>Leetcode</category>
        
        <category>Tree</category>
        
        
        <category>blog</category>
        
      </item>
    
      <item>
        <title>CNN 读书笔记（4） Batch Normalization</title>
        <description>&lt;h2 id=&quot;batch-normalization&quot;&gt;Batch Normalization&lt;/h2&gt;

&lt;h3 id=&quot;inutition&quot;&gt;&lt;strong&gt;Inutition&lt;/strong&gt;:&lt;/h3&gt;

&lt;p&gt;Machine learning methods tend to work better when their input data consists of uncorrelated features with zero mean and unit variance.&lt;/p&gt;

&lt;h3 id=&quot;problem&quot;&gt;&lt;strong&gt;Problem&lt;/strong&gt;:&lt;/h3&gt;

&lt;p&gt;Even if we preprocess the input data, the activations at deeper layers of the network will likely no longer be decorrelated and will no longer have zero mean or unit variance since they are output from earlier layers in the network.&lt;/p&gt;

&lt;h3 id=&quot;methods&quot;&gt;&lt;strong&gt;Methods&lt;/strong&gt;:&lt;/h3&gt;

&lt;p&gt;At training time, a &lt;strong&gt;batch normalization layer&lt;/strong&gt; uses a minibatch of data to estimate the mean and standard deviation of each feature. These estimated means and standard deviations are then used to center and normalize the features of the minibatch. A &lt;strong&gt;running average of these means&lt;/strong&gt; and standard deviations is kept during training, and at test time these running averages are used to center and normalize features.&lt;/p&gt;

&lt;h3 id=&quot;show-the-code&quot;&gt;Show the code:&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;def batchnorm_forward(x, gamma, beta, bn_param):
  &quot;&quot;&quot;
  Forward pass for batch normalization.

  During training the sample mean and (uncorrected) sample variance are
  computed from minibatch statistics and used to normalize the incoming data.
  During training we also keep an exponentially decaying running mean of the mean
  and variance of each feature, and these averages are used to normalize data
  at test-time.

  At each timestep we update the running averages for mean and variance using
  an exponential decay based on the momentum parameter:

  running_mean = momentum * running_mean + (1 - momentum) * sample_mean
  running_var = momentum * running_var + (1 - momentum) * sample_var
  Input:
  - x: Data of shape (N, D)
  - gamma: Scale parameter of shape (D,)
  - beta: Shift paremeter of shape (D,)
  - bn_param: Dictionary with the following keys:
    - mode: &#39;train&#39; or &#39;test&#39;; required
    - eps: Constant for numeric stability
    - momentum: Constant for running mean / variance.
    - running_mean: Array of shape (D,) giving running mean of features
    - running_var Array of shape (D,) giving running variance of features

  Returns a tuple of:
  - out: of shape (N, D)
  - cache: A tuple of values needed in the backward pass
  &quot;&quot;&quot;
  mode = bn_param[&#39;mode&#39;]
  eps = bn_param.get(&#39;eps&#39;, 1e-5)
  momentum = bn_param.get(&#39;momentum&#39;, 0.9)

  N, D = x.shape
  running_mean = bn_param.get(&#39;running_mean&#39;, np.zeros(D, dtype=x.dtype))
  running_var = bn_param.get(&#39;running_var&#39;, np.zeros(D, dtype=x.dtype))

  out, cache = None, None
  if mode == &#39;train&#39;:
      sample_mean = np.mean(x, axis=0)
      sample_var  = np.var(x, axis=0)

      running_mean = momentum * running_mean + (1 - momentum) * sample_mean
      running_var = momentum * running_var + (1 - momentum) * sample_var

      x_normed = (x - sample_mean) / np.sqrt(sample_var + eps)
      y = x_normed * gamma + beta

      out = y

      cache = (x, x_normed, gamma, beta, sample_mean, sample_var, eps)

  elif mode == &#39;test&#39;:
      x_normed = (x - running_mean) / np.sqrt(running_var + eps)
      y = gamma * x_normed + beta

      out = y
  else:
      raise ValueError(&#39;Invalid forward batchnorm mode &quot;%s&quot;&#39; % mode)

  # Store the updated running means back into bn_param
  bn_param[&#39;running_mean&#39;] = running_mean
  bn_param[&#39;running_var&#39;] = running_var

  return out, cache

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;### Backprogogation in Batch Normalization&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;
def batchnorm_backward(dout, cache):
  &quot;&quot;&quot;
  Backward pass for batch normalization.


  Inputs:
  - dout: Upstream derivatives, of shape (N, D)
  - cache: Variable of intermediates from batchnorm_forward.

  Returns a tuple of:
  - dx: Gradient with respect to inputs x, of shape (N, D)
  - dgamma: Gradient with respect to scale parameter gamma, of shape (D,)
  - dbeta: Gradient with respect to shift parameter beta, of shape (D,)
  &quot;&quot;&quot;

  x, x_normed,gamma,beta, mean, var, eps = cache

  N = x.shape[0]


  dbeta = np.sum(dout, axis=0)
  dgamma = np.sum(dout * x_normed, axis=0)
  dx_normed = gamma * dout
  dx_std = np.sum(-1.0/2*dx_normed*(x-mean)  / (var+eps)**(3.0/2), axis=0)
  dx_mean = np.sum(dx_normed * -1.0 / np.sqrt(var+eps), axis=0) + 1.0/N * dx_std*np.sum(-2*(x - mean), axis=0)

  dx = dx_normed*1/np.sqrt(var+eps) + dx_std * 2.0/N*(x-mean)+ 1.0/N * dx_mean

  return dx, dgamma, dbeta

&lt;/code&gt;&lt;/pre&gt;

</description>
        <pubDate>Tue, 02 Aug 2016 00:00:00 +0800</pubDate>
        <link>samshaq19912009.github.io/blog/2016/08/02/CNN_Bacth_Normal/</link>
        <guid isPermaLink="true">samshaq19912009.github.io/blog/2016/08/02/CNN_Bacth_Normal/</guid>
        
        <category>Machine Learning</category>
        
        <category>Notes</category>
        
        <category>CNN</category>
        
        
        <category>blog</category>
        
      </item>
    
      <item>
        <title>CNN 读书笔记（4） Netural Network 初入门(2)</title>
        <description>&lt;h2 id=&quot;data-preprocessing&quot;&gt;Data Preprocessing&lt;/h2&gt;

&lt;p&gt;Input vector: $X [ N * D ]$&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Mean subtraction&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;X -= np.mean(X, axis = 0)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Normalization&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;(X /= np.std(X, axis = 0)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;http://cs231n.github.io/assets/nn2/prepro1.jpeg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;PCA and weighting&lt;/strong&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Compute the covariance matrix&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# Assume input data matrix X of size [N x D]
X -= np.mean(X, axis = 0) # zero-center the data (important)
cov = np.dot(X.T, X) / X.shape[0] # get the data covariance matrix

&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;Perform singluar value decomposition&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;the columns of U are eigenvectors. S is the eigenvalues&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;U,S,V = np.linalg.svd(cov)

Xrot = np.dot(X, U) # decorrelate the data

Xrot_reduced = np.dot(X, U[:,:100]) # Xrot_reduced becomes [N x 100]


&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;blockquote&gt;
  &lt;p&gt;Weightening&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# whiten the data:
# divide by the eigenvalues (which are square roots of the singular values)
Xwhite = Xrot / np.sqrt(S + 1e-5)

&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;http://cs231n.github.io/assets/nn2/prepro2.jpeg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;In practise&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;No PCA and weighten, only zero-centered the data&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Common Pitfall&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;An important point to make about the preprocessing is that any preprocessing statistics (e.g. the data mean) must only be &lt;strong&gt;computed on the training data&lt;/strong&gt;, and then applied to the validation / test data. E.g. computing the mean and subtracting it from every image across the entire dataset and then splitting the data into train/val/test splits would be a mistake. Instead, the mean must be computed &lt;strong&gt;only over the training data&lt;/strong&gt; and then &lt;strong&gt;subtracted equally from all splits&lt;/strong&gt; (train/val/test).&lt;/p&gt;

&lt;h2 id=&quot;weight-initialization&quot;&gt;Weight Initialization&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Pitfall: all zero initialization&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;There is no source of asymmetry between neurons if their weights are initialized to be the same.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Small random numbers with variance calibraion&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;w = np.random.randn(n) / sqrt(n)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Initution: Output $s = \sum_i^n w_i x_i$ has the same variance&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
% &lt;![CDATA[
\begin{align}
\text{Var}(s) &amp;= \text{Var}(\sum_i^n w_ix_i) \\\\
&amp;= \sum_i^n \text{Var}(w_ix_i) \\\\
&amp;= \sum_i^n [E(w_i)]^2\text{Var}(x_i) + E[(x_i)]^2\text{Var}(w_i) + \text{Var}(x_i)\text{Var}(w_i) \\\\
&amp;= \sum_i^n \text{Var}(x_i)\text{Var}(w_i) \\\\
&amp;= \left( n \text{Var}(w) \right) \text{Var}(x)
\end{align} %]]&gt; %]]&gt;&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;In practise with Relu activation&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;w = np.random.randn(n) * sqrt(2.0/n)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h2 id=&quot;regularization&quot;&gt;Regularization&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;L2 regularization&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;L1 regularization&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;In practice, if you are not concerned with explicit feature selection, L2 regularization can be expected to give superior performance over L1.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Drouout&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://cs231n.github.io/assets/nn2/dropout.jpeg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;loss-function&quot;&gt;Loss function&lt;/h2&gt;

&lt;h3 id=&quot;classification&quot;&gt;&lt;strong&gt;Classification&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;SVM Loss&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;$L_i = \sum_{j\neq y_i} \max(0, f_j - f_{y_i} + 1)$&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Softmax Loss&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;$L_i = -\log\left(\frac{e^{f_{y_i}}}{ \sum_j e^{f_j} }\right)$&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Cross-entropy Loss&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;$L_i = \sum_j y_{ij} \log(\sigma(f_j)) + (1 - y_{ij}) \log(1 - \sigma(f_j))$&lt;/p&gt;

&lt;h3 id=&quot;regression&quot;&gt;Regression&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;L2 norm&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;$L_i = \Vert f - y_i \Vert_2^2$&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Caution&lt;/strong&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;When faced with a regression task, first consider if it is absolutely necessary. Instead, have a strong preference to discretizing your outputs to bins and perform classification over them whenever possible.&lt;/p&gt;
&lt;/blockquote&gt;
</description>
        <pubDate>Mon, 01 Aug 2016 00:00:00 +0800</pubDate>
        <link>samshaq19912009.github.io/blog/2016/08/01/CNN_Data-Preprocessing/</link>
        <guid isPermaLink="true">samshaq19912009.github.io/blog/2016/08/01/CNN_Data-Preprocessing/</guid>
        
        <category>Machine Learning</category>
        
        <category>Notes</category>
        
        <category>CNN</category>
        
        
        <category>blog</category>
        
      </item>
    
      <item>
        <title>Best Time to Buy with Cooldown</title>
        <description>&lt;p&gt;&lt;a href=&quot;https://leetcode.com/problems/best-time-to-buy-and-sell-stock-with-cooldown/&quot;&gt;Link&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Say you have an array for which the ith element is the price of a given stock on day i.&lt;/p&gt;

&lt;p&gt;Design an algorithm to find the maximum profit. You may complete as many transactions as you like ( buy one and sell one share of the stock multiple times) with the following restrictions:&lt;/p&gt;

&lt;p&gt;You may not engage in multiple transactions at the same time ( you must sell the stock before you buy again).
After you sell your stock, you cannot buy stock on next day. ( cooldown 1 day)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;prices = [1, 2, 3, 0, 2]&lt;/p&gt;

&lt;p&gt;maxProfit = 3&lt;/p&gt;

&lt;p&gt;transactions = [buy, sell, cooldown, buy, sell]&lt;/p&gt;

&lt;p&gt;Solution:&lt;/p&gt;

&lt;p&gt;Then the transaction sequences can end with any of these three states.&lt;/p&gt;

&lt;p&gt;For each of them we make an array, buy[n], sell[n] and rest[n].&lt;/p&gt;

&lt;p&gt;buy[i] means before day i what is the maxProfit for any sequence end with buy.&lt;/p&gt;

&lt;p&gt;sell[i] means before day i what is the maxProfit for any sequence end with sell.&lt;/p&gt;

&lt;p&gt;rest[i] means before day i what is the maxProfit for any sequence end with rest.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;buy[i] = max(buy[i-1], reset[i-1]-price)

reset[i] = max(reset[i-1], sell[i-1], buy[i-1])

sell[i] = max(buy[i-1]+price, sell[i-1])
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;To make sure buy-reset-buy is not going to happen, observe that&lt;/p&gt;

&lt;p&gt;reset[i] &amp;gt;= buy[i-1], so&lt;/p&gt;

&lt;p&gt;reset[i] = max(reset[i-1], sell[i-1])&lt;/p&gt;

&lt;p&gt;reset[i] &amp;lt;= sell[i] =&amp;gt; reset[i] == sell[i-1]&lt;/p&gt;

&lt;p&gt;so&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;buy[i] = max(buy[i-1], sell[i-2]-price)

sell[i] = max(sell[i-1], buy[i-1]+price)


&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;The final solution is&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;class Solution(object):
    def maxProfit(self, prices):
        &quot;&quot;&quot;
        :type prices: List[int]
        :rtype: int
        &quot;&quot;&quot;
        if len(prices) &amp;lt; 2:
            return 0
        sell,buy = 0,-prices[0]
        pre_sell, pre_buy = 0,0
        for price in prices:
            pre_buy = buy
            buy = max(pre_sell - price, pre_buy)
            pre_sell = sell
            sell = max(pre_buy + price, pre_sell)
            
        return sell


&lt;/code&gt;&lt;/pre&gt;

</description>
        <pubDate>Thu, 28 Jul 2016 00:00:00 +0800</pubDate>
        <link>samshaq19912009.github.io/blog/2016/07/28/Best_time_buy_with-cooldown_leetcode/</link>
        <guid isPermaLink="true">samshaq19912009.github.io/blog/2016/07/28/Best_time_buy_with-cooldown_leetcode/</guid>
        
        <category>Leetcode</category>
        
        <category>Array</category>
        
        
        <category>blog</category>
        
      </item>
    
      <item>
        <title>Leetcode Spiral Matrix</title>
        <description>&lt;p&gt;&lt;a href=&quot;https://leetcode.com/problems/spiral-matrix/&quot;&gt;Link&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;class Solution(object):
    def spiralOrder(self, matrix):
        &quot;&quot;&quot;
        :type matrix: List[List[int]]
        :rtype: List[int]
        &quot;&quot;&quot;
        if not matrix:
            return []
        m, n = len(matrix), len(matrix[0])
        ans = []
        x_start, x_end = 0, n-1
        y_start, y_end = 0, m-1
        while True:
            ##from left to right
            for i in range(x_start, x_end+1):
                ans.append(matrix[y_start][i])
            ##from top to bottom
            y_start += 1
            if y_start &amp;gt; y_end:
                break
            
            for j in range(y_start, y_end+1):
                ans.append(matrix[j][x_end])
            ##from right t0 left
            x_end -= 1
            if x_end &amp;lt; x_start:
                break
            for i in range(x_end, x_start-1, -1):
                ans.append(matrix[y_end][i])
            ##from bottom to top
            y_end -=1
            if y_end &amp;lt; y_start:
                break
            for j in range(y_end, y_start-1, -1):
                ans.append(matrix[j][x_start])
            x_start += 1
            if x_start &amp;gt; x_end:
                break
        return ans
                


&lt;/code&gt;&lt;/pre&gt;
</description>
        <pubDate>Thu, 28 Jul 2016 00:00:00 +0800</pubDate>
        <link>samshaq19912009.github.io/blog/2016/07/28/Sipral_matrix_leetcode/</link>
        <guid isPermaLink="true">samshaq19912009.github.io/blog/2016/07/28/Sipral_matrix_leetcode/</guid>
        
        <category>Leetcode</category>
        
        <category>Array</category>
        
        
        <category>blog</category>
        
      </item>
    
      <item>
        <title>Leetcode Merge K List</title>
        <description>&lt;p&gt;&lt;a href=&quot;https://leetcode.com/problems/merge-k-sorted-lists/&quot;&gt;Link&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;# Definition for singly-linked list.
# class ListNode(object):
#     def __init__(self, x):
#         self.val = x
#         self.next = None

class Solution(object):
    def mergeKLists(self, lists):
        &quot;&quot;&quot;
        :type lists: List[ListNode]
        :rtype: ListNode
        &quot;&quot;&quot;
        def merge(l1, l2):
            dummy = ListNode(-1)
            cur = dummy
            while l1 and l2:
                if l1.val &amp;lt; l2.val:
                    cur.next = l1
                    l1 = l1.next
                else:
                    cur.next = l2
                    l2 = l2.next
                cur = cur.next
            cur.next = l1 or l2
            return dummy.next
        def helper(lists,begin, end):
            if begin &amp;gt; end:
                return 
            if begin == end:
                return lists[begin]
            mid = (begin+end)/2
            return merge(helper(lists,begin, mid) , helper(lists, mid+1, end))
        return helper(lists, 0, len(lists)-1)


&lt;/code&gt;&lt;/pre&gt;
</description>
        <pubDate>Thu, 28 Jul 2016 00:00:00 +0800</pubDate>
        <link>samshaq19912009.github.io/blog/2016/07/28/Merge_K_list_leetcode/</link>
        <guid isPermaLink="true">samshaq19912009.github.io/blog/2016/07/28/Merge_K_list_leetcode/</guid>
        
        <category>Leetcode</category>
        
        <category>sort</category>
        
        
        <category>blog</category>
        
      </item>
    
      <item>
        <title>Leetcode Find Kth Largest</title>
        <description>&lt;p&gt;&lt;a href=&quot;https://leetcode.com/problems/kth-largest-element-in-an-array/&quot;&gt;Link&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;class Solution(object):
    def findKthLargest(self, nums, k):
        &quot;&quot;&quot;
        :type nums: List[int]
        :type k: int
        :rtype: int
        &quot;&quot;&quot;
        pivot = nums[0]
        tail = 0
        for i in range(1, len(nums)):
            if nums[i] &amp;gt; pivot:
                tail += 1
                nums[i], nums[tail] = nums[tail], nums[i]
        nums[0], nums[tail] = nums[tail], nums[0]
        
        if tail+ 1 == k:
            return pivot
        elif tail+1 &amp;lt; k:
            return self.findKthLargest(nums[tail+1:], k-tail-1)
        else:
            return self.findKthLargest(nums[:tail], k)
            


&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;randomnized-quick-select&quot;&gt;Randomnized Quick Select&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;
import random
class Solution(object):
    def findKthLargest(self, nums, k):
        &quot;&quot;&quot;
        :type nums: List[int]
        :type k: int
        :rtype: int
        &quot;&quot;&quot;
        pivot = random.choice(nums)
        num1, num2 = [], []
        for num in nums:
            if num &amp;gt; pivot:
                num1.append(num)
            elif num &amp;lt; pivot:
                num2.append(num)
        if k &amp;lt;= len(num1):
            return self.findKthLargest(num1, k)
        elif k &amp;gt; len(nums) - len(num2):
            return self.findKthLargest(num2, k- (len(nums) - len(num2)))
        
        return pivot


&lt;/code&gt;&lt;/pre&gt;
</description>
        <pubDate>Thu, 28 Jul 2016 00:00:00 +0800</pubDate>
        <link>samshaq19912009.github.io/blog/2016/07/28/Find_Kth_leetcode/</link>
        <guid isPermaLink="true">samshaq19912009.github.io/blog/2016/07/28/Find_Kth_leetcode/</guid>
        
        <category>Leetcode</category>
        
        <category>sort</category>
        
        
        <category>blog</category>
        
      </item>
    
  </channel>
</rss>
