<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Chengcen</title>
    <description></description>
    <link>samshaq19912009.github.io/</link>
    <atom:link href="samshaq19912009.github.io/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Thu, 11 Aug 2016 01:49:52 +0800</pubDate>
    <lastBuildDate>Thu, 11 Aug 2016 01:49:52 +0800</lastBuildDate>
    <generator>Jekyll v3.1.6</generator>
    
      <item>
        <title>Binary Tree Level Order Traversal</title>
        <description>&lt;p&gt;&lt;a href=&quot;https://leetcode.com/problems/binary-tree-level-order-traversal/&quot;&gt;Leetcode Link&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;solution-1--using-queue&quot;&gt;Solution 1 : Using Queue&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;class Solution(object):
    def levelOrder(self, root):
        &quot;&quot;&quot;
        :type root: TreeNode
        :rtype: List[List[int]]
        &quot;&quot;&quot;
        if not root:
            return []
        q = [root]
        ans = []
        while q:
            ans.append([node.val for node in q])
            new_q = []
            for node in q:
                if node.left:
                    new_q.append(node.left)
                if node.right:
                    new_q.append(node.right)
            q = new_q
        return ans

&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;solution-2-depth-first-search&quot;&gt;Solution 2: Depth First Search&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;class Solution(object):
    def levelOrder(self, root):
        &quot;&quot;&quot;
        :type root: TreeNode
        :rtype: List[List[int]]
        &quot;&quot;&quot;
        if not root:
            return []
        ans = []
        self.bfs(root,0, ans)
        return ans
        
    def bfs(self, node, level, ans):
        if not node:
            return
        
        if len(ans) &amp;lt; level+1:
            ans.append([])
        ans[level].append(node.val)
        self.bfs(node.left, level+1, ans)
        self.bfs(node.right, level+1, ans)


&lt;/code&gt;&lt;/pre&gt;
</description>
        <pubDate>Tue, 09 Aug 2016 00:00:00 +0800</pubDate>
        <link>samshaq19912009.github.io/blog/2016/08/09/Course_Schedule_leetcode/</link>
        <guid isPermaLink="true">samshaq19912009.github.io/blog/2016/08/09/Course_Schedule_leetcode/</guid>
        
        <category>Leetcode</category>
        
        <category>Tree</category>
        
        
        <category>blog</category>
        
      </item>
    
      <item>
        <title>Best Time to Buy with Cooldown</title>
        <description>&lt;p&gt;&lt;a href=&quot;https://leetcode.com/problems/best-time-to-buy-and-sell-stock-with-cooldown/&quot;&gt;Link&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Say you have an array for which the ith element is the price of a given stock on day i.&lt;/p&gt;

&lt;p&gt;Design an algorithm to find the maximum profit. You may complete as many transactions as you like ( buy one and sell one share of the stock multiple times) with the following restrictions:&lt;/p&gt;

&lt;p&gt;You may not engage in multiple transactions at the same time ( you must sell the stock before you buy again).
After you sell your stock, you cannot buy stock on next day. ( cooldown 1 day)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;prices = [1, 2, 3, 0, 2]&lt;/p&gt;

&lt;p&gt;maxProfit = 3&lt;/p&gt;

&lt;p&gt;transactions = [buy, sell, cooldown, buy, sell]&lt;/p&gt;

&lt;p&gt;Solution:&lt;/p&gt;

&lt;p&gt;Then the transaction sequences can end with any of these three states.&lt;/p&gt;

&lt;p&gt;For each of them we make an array, buy[n], sell[n] and rest[n].&lt;/p&gt;

&lt;p&gt;buy[i] means before day i what is the maxProfit for any sequence end with buy.&lt;/p&gt;

&lt;p&gt;sell[i] means before day i what is the maxProfit for any sequence end with sell.&lt;/p&gt;

&lt;p&gt;rest[i] means before day i what is the maxProfit for any sequence end with rest.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;buy[i] = max(buy[i-1], reset[i-1]-price)

reset[i] = max(reset[i-1], sell[i-1], buy[i-1])

sell[i] = max(buy[i-1]+price, sell[i-1])
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;To make sure buy-reset-buy is not going to happen, observe that&lt;/p&gt;

&lt;p&gt;reset[i] &amp;gt;= buy[i-1], so&lt;/p&gt;

&lt;p&gt;reset[i] = max(reset[i-1], sell[i-1])&lt;/p&gt;

&lt;p&gt;reset[i] &amp;lt;= sell[i] =&amp;gt; reset[i] == sell[i-1]&lt;/p&gt;

&lt;p&gt;so&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;buy[i] = max(buy[i-1], sell[i-2]-price)

sell[i] = max(sell[i-1], buy[i-1]+price)


&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;The final solution is&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;class Solution(object):
    def maxProfit(self, prices):
        &quot;&quot;&quot;
        :type prices: List[int]
        :rtype: int
        &quot;&quot;&quot;
        if len(prices) &amp;lt; 2:
            return 0
        sell,buy = 0,-prices[0]
        pre_sell, pre_buy = 0,0
        for price in prices:
            pre_buy = buy
            buy = max(pre_sell - price, pre_buy)
            pre_sell = sell
            sell = max(pre_buy + price, pre_sell)
            
        return sell


&lt;/code&gt;&lt;/pre&gt;

</description>
        <pubDate>Thu, 28 Jul 2016 00:00:00 +0800</pubDate>
        <link>samshaq19912009.github.io/blog/2016/07/28/Best_time_buy_with-cooldown_leetcode/</link>
        <guid isPermaLink="true">samshaq19912009.github.io/blog/2016/07/28/Best_time_buy_with-cooldown_leetcode/</guid>
        
        <category>Leetcode</category>
        
        <category>Array</category>
        
        
        <category>blog</category>
        
      </item>
    
      <item>
        <title>Leetcode Spiral Matrix</title>
        <description>&lt;p&gt;&lt;a href=&quot;https://leetcode.com/problems/spiral-matrix/&quot;&gt;Link&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;class Solution(object):
    def spiralOrder(self, matrix):
        &quot;&quot;&quot;
        :type matrix: List[List[int]]
        :rtype: List[int]
        &quot;&quot;&quot;
        if not matrix:
            return []
        m, n = len(matrix), len(matrix[0])
        ans = []
        x_start, x_end = 0, n-1
        y_start, y_end = 0, m-1
        while True:
            ##from left to right
            for i in range(x_start, x_end+1):
                ans.append(matrix[y_start][i])
            ##from top to bottom
            y_start += 1
            if y_start &amp;gt; y_end:
                break
            
            for j in range(y_start, y_end+1):
                ans.append(matrix[j][x_end])
            ##from right t0 left
            x_end -= 1
            if x_end &amp;lt; x_start:
                break
            for i in range(x_end, x_start-1, -1):
                ans.append(matrix[y_end][i])
            ##from bottom to top
            y_end -=1
            if y_end &amp;lt; y_start:
                break
            for j in range(y_end, y_start-1, -1):
                ans.append(matrix[j][x_start])
            x_start += 1
            if x_start &amp;gt; x_end:
                break
        return ans
                


&lt;/code&gt;&lt;/pre&gt;
</description>
        <pubDate>Thu, 28 Jul 2016 00:00:00 +0800</pubDate>
        <link>samshaq19912009.github.io/blog/2016/07/28/Sipral_matrix_leetcode/</link>
        <guid isPermaLink="true">samshaq19912009.github.io/blog/2016/07/28/Sipral_matrix_leetcode/</guid>
        
        <category>Leetcode</category>
        
        <category>Array</category>
        
        
        <category>blog</category>
        
      </item>
    
      <item>
        <title>Leetcode Merge K List</title>
        <description>&lt;p&gt;&lt;a href=&quot;https://leetcode.com/problems/merge-k-sorted-lists/&quot;&gt;Link&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;# Definition for singly-linked list.
# class ListNode(object):
#     def __init__(self, x):
#         self.val = x
#         self.next = None

class Solution(object):
    def mergeKLists(self, lists):
        &quot;&quot;&quot;
        :type lists: List[ListNode]
        :rtype: ListNode
        &quot;&quot;&quot;
        def merge(l1, l2):
            dummy = ListNode(-1)
            cur = dummy
            while l1 and l2:
                if l1.val &amp;lt; l2.val:
                    cur.next = l1
                    l1 = l1.next
                else:
                    cur.next = l2
                    l2 = l2.next
                cur = cur.next
            cur.next = l1 or l2
            return dummy.next
        def helper(lists,begin, end):
            if begin &amp;gt; end:
                return 
            if begin == end:
                return lists[begin]
            mid = (begin+end)/2
            return merge(helper(lists,begin, mid) , helper(lists, mid+1, end))
        return helper(lists, 0, len(lists)-1)


&lt;/code&gt;&lt;/pre&gt;
</description>
        <pubDate>Thu, 28 Jul 2016 00:00:00 +0800</pubDate>
        <link>samshaq19912009.github.io/blog/2016/07/28/Merge_K_list_leetcode/</link>
        <guid isPermaLink="true">samshaq19912009.github.io/blog/2016/07/28/Merge_K_list_leetcode/</guid>
        
        <category>Leetcode</category>
        
        <category>sort</category>
        
        
        <category>blog</category>
        
      </item>
    
      <item>
        <title>Leetcode Find Kth Largest</title>
        <description>&lt;p&gt;&lt;a href=&quot;https://leetcode.com/problems/kth-largest-element-in-an-array/&quot;&gt;Link&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;class Solution(object):
    def findKthLargest(self, nums, k):
        &quot;&quot;&quot;
        :type nums: List[int]
        :type k: int
        :rtype: int
        &quot;&quot;&quot;
        pivot = nums[0]
        tail = 0
        for i in range(1, len(nums)):
            if nums[i] &amp;gt; pivot:
                tail += 1
                nums[i], nums[tail] = nums[tail], nums[i]
        nums[0], nums[tail] = nums[tail], nums[0]
        
        if tail+ 1 == k:
            return pivot
        elif tail+1 &amp;lt; k:
            return self.findKthLargest(nums[tail+1:], k-tail-1)
        else:
            return self.findKthLargest(nums[:tail], k)
            


&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;randomnized-quick-select&quot;&gt;Randomnized Quick Select&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;
import random
class Solution(object):
    def findKthLargest(self, nums, k):
        &quot;&quot;&quot;
        :type nums: List[int]
        :type k: int
        :rtype: int
        &quot;&quot;&quot;
        pivot = random.choice(nums)
        num1, num2 = [], []
        for num in nums:
            if num &amp;gt; pivot:
                num1.append(num)
            elif num &amp;lt; pivot:
                num2.append(num)
        if k &amp;lt;= len(num1):
            return self.findKthLargest(num1, k)
        elif k &amp;gt; len(nums) - len(num2):
            return self.findKthLargest(num2, k- (len(nums) - len(num2)))
        
        return pivot


&lt;/code&gt;&lt;/pre&gt;
</description>
        <pubDate>Thu, 28 Jul 2016 00:00:00 +0800</pubDate>
        <link>samshaq19912009.github.io/blog/2016/07/28/Find_Kth_leetcode/</link>
        <guid isPermaLink="true">samshaq19912009.github.io/blog/2016/07/28/Find_Kth_leetcode/</guid>
        
        <category>Leetcode</category>
        
        <category>sort</category>
        
        
        <category>blog</category>
        
      </item>
    
      <item>
        <title>Leetcode Multiply String</title>
        <description>&lt;p&gt;&lt;a href=&quot;https://leetcode.com/problems/multiply-strings/&quot;&gt;Link&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Given two numbers represented as strings, return multiplication of the numbers as a string.&lt;/p&gt;

&lt;p&gt;Note:
The numbers can be arbitrarily large and are non-negative.
Converting the input string to integer is NOT allowed.
You should NOT use internal library such as BigInteger.&lt;/p&gt;

&lt;p&gt;题目解析：&lt;/p&gt;

&lt;p&gt;先将字符串翻转，从末尾开始计数。计算临时结果，先不要进位。&lt;/p&gt;

&lt;p&gt;然后开始计算进位，每次保留carry向前一位。在ans的开头插入每一步的结果。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;class Solution(object):
    def multiply(self, num1, num2):
        &quot;&quot;&quot;
        :type num1: str
        :type num2: str
        :rtype: str
        &quot;&quot;&quot;
        num1 = num1[::-1]
        num2 = num2[::-1]
        ans = [0] * (len(num1)+len(num2))
        for i in range(len(num1)):
            for j in range(len(num2)):
                ans[i+j] += int(num1[i]) * int(num2[j])
        
        result = []
        for i in range(len(ans)):
            digit = ans[i] % 10
            carry = ans[i] / 10
            if i &amp;lt; len(ans) - 1:
                ans[i+1] += carry
            result.insert(0, str(digit))
        while result[0] == &#39;0&#39; and len(result) &amp;gt; 1:
            del result[0]
        return &quot;&quot;.join(result)
            


&lt;/code&gt;&lt;/pre&gt;
</description>
        <pubDate>Wed, 27 Jul 2016 00:00:00 +0800</pubDate>
        <link>samshaq19912009.github.io/blog/2016/07/27/Multiply_String_leetcode/</link>
        <guid isPermaLink="true">samshaq19912009.github.io/blog/2016/07/27/Multiply_String_leetcode/</guid>
        
        <category>Leetcode</category>
        
        <category>Math</category>
        
        
        <category>blog</category>
        
      </item>
    
      <item>
        <title>Leetcode Design Twitter</title>
        <description>&lt;p&gt;&lt;a href=&quot;https://leetcode.com/problems/design-twitter/&quot;&gt;Link&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Design a simplified version of Twitter where users can post tweets, follow/unfollow another user and is able to see the 10 most recent tweets in the user’s news feed. Your design should support the following methods:&lt;/p&gt;

&lt;p&gt;postTweet(userId, tweetId): Compose a new tweet.
getNewsFeed(userId): Retrieve the 10 most recent tweet ids in the user’s news feed. Each item in the news feed must be posted by users who the user followed or by the user herself. Tweets must be ordered from most recent to least recent.&lt;/p&gt;

&lt;p&gt;follow(followerId, followeeId): Follower follows a followee.&lt;/p&gt;

&lt;p&gt;unfollow(followerId, followeeId): Follower unfollows a followee.&lt;/p&gt;

&lt;p&gt;Solution:&lt;/p&gt;

&lt;p&gt;Use deque to store and tweets&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;
class Twitter(object):

    def __init__(self):
        &quot;&quot;&quot;
        Initialize your data structure here.
        &quot;&quot;&quot;
        self.tweets = collections.defaultdict(collections.deque)
        self.followees = collections.defaultdict(set)
        self.timer = itertools.count(step=-1)
        

    def postTweet(self, userId, tweetId):
        &quot;&quot;&quot;
        Compose a new tweet.
        :type userId: int
        :type tweetId: int
        :rtype: void
        &quot;&quot;&quot;
        self.tweets[userId].appendleft((next(self.timer), tweetId))
        

    def getNewsFeed(self, userId):
        &quot;&quot;&quot;
        Retrieve the 10 most recent tweet ids in the user&#39;s news feed. Each item in the news feed must be posted by users who the user followed or by the user herself. Tweets must be ordered from most recent to least recent.
        :type userId: int
        :rtype: List[int]
        &quot;&quot;&quot;
        tweets = heapq.merge(*(self.tweets[u] for u in self.followees[userId] | {userId}))
        return [t for _, t in itertools.islice(tweets, 10)]
        

    def follow(self, followerId, followeeId):
        &quot;&quot;&quot;
        Follower follows a followee. If the operation is invalid, it should be a no-op.
        :type followerId: int
        :type followeeId: int
        :rtype: void
        &quot;&quot;&quot;
        self.followees[followerId].add(followeeId)
        

    def unfollow(self, followerId, followeeId):
        &quot;&quot;&quot;
        Follower unfollows a followee. If the operation is invalid, it should be a no-op.
        :type followerId: int
        :type followeeId: int
        :rtype: void
        &quot;&quot;&quot;
        self.followees[followerId].discard(followeeId)


# Your Twitter object will be instantiated and called as such:
# obj = Twitter()
# obj.postTweet(userId,tweetId)
# param_2 = obj.getNewsFeed(userId)
# obj.follow(followerId,followeeId)
# obj.unfollow(followerId,followeeId)


&lt;/code&gt;&lt;/pre&gt;
</description>
        <pubDate>Wed, 27 Jul 2016 00:00:00 +0800</pubDate>
        <link>samshaq19912009.github.io/blog/2016/07/27/Design_twitter_leetcode/</link>
        <guid isPermaLink="true">samshaq19912009.github.io/blog/2016/07/27/Design_twitter_leetcode/</guid>
        
        <category>Leetcode</category>
        
        <category>Design</category>
        
        
        <category>blog</category>
        
      </item>
    
      <item>
        <title>Leetcode Super Ugly Number</title>
        <description>&lt;p&gt;&lt;a href=&quot;https://leetcode.com/problems/super-ugly-number/&quot;&gt;Link&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Write a program to find the nth super ugly number.&lt;/p&gt;

&lt;p&gt;Super ugly numbers are positive numbers whose all prime factors are in the given prime list primes of size k. For example, [1, 2, 4, 7, 8, 13, 14, 16, 19, 26, 28, 32] is the sequence of the first 12 super ugly numbers given primes = [2, 7, 13, 19] of size 4.&lt;/p&gt;

&lt;p&gt;Note:
(1) 1 is a super ugly number for any given primes.
(2) The given numbers in primes are in ascending order.
(3) 0 &amp;lt; k ≤ 100, 0 &amp;lt; n ≤ 106, 0 &amp;lt; primes[i] &amp;lt; 1000.&lt;/p&gt;

&lt;p&gt;题目解析：&lt;/p&gt;

&lt;p&gt;可以使用&lt;a href=&quot;https://leetcode.com/problems/ugly-number-ii/&quot;&gt;Ugly Number II&lt;/a&gt;的解法，开辟一个index的序列记录primen number的下标。每次更新选择其中一个最小的，但这样的解法太慢。比较好的是利用heapq和yield 生成器。具体代码如下&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;class Solution(object):
    def nthSuperUglyNumber(self, n, primes):
        &quot;&quot;&quot;
        :type n: int
        :type primes: List[int]
        :rtype: int
        &quot;&quot;&quot;
        ans = [1]
        def gen(prime):
            for x in ans:
                yield x * prime
        merged = heapq.merge(*map(gen, primes))
        while len(ans) &amp;lt; n:
            ugly = next(merged)
            if ugly != ans[-1]:
                ans.append(ugly)
        return ans[-1]

&lt;/code&gt;&lt;/pre&gt;
</description>
        <pubDate>Tue, 26 Jul 2016 00:00:00 +0800</pubDate>
        <link>samshaq19912009.github.io/blog/2016/07/26/Super_ugly_number_leetcode/</link>
        <guid isPermaLink="true">samshaq19912009.github.io/blog/2016/07/26/Super_ugly_number_leetcode/</guid>
        
        <category>Leetcode</category>
        
        <category>heap</category>
        
        
        <category>blog</category>
        
      </item>
    
      <item>
        <title>CNN 读书笔记（3） Netural Network 初入门</title>
        <description>&lt;h3 id=&quot;different-activation-function&quot;&gt;Different Activation Function&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;http://cs231n.github.io/assets/nn1/sigmoid.jpeg&quot; alt=&quot;Two common activation function&quot; /&gt;&lt;img src=&quot;http://cs231n.github.io/assets/nn1/tanh.jpeg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;drawback-for-sigmod-function&quot;&gt;Drawback for sigmod function&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;Sigmoids saturate and kill gradients&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Sigmoid outputs are not zero-centered.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Tanh function&lt;/strong&gt; is zero-centered&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Why Relu&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Converge faster&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Simply calculation&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Unfortunately, ReLU units can be fragile during training and can “die”&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;neural-network-architectures&quot;&gt;Neural Network architectures&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;http://cs231n.github.io/assets/nn1/neural_net.jpeg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;representational-power&quot;&gt;Representational power&lt;/h3&gt;

&lt;p&gt;Neural Networks with at least one hidden layer are universal approximators :&lt;/p&gt;

&lt;p&gt;given any continuous function f(x) and some ϵ&amp;gt;0 , there exists a Neural Network g(x) with one hidden layer (with a reasonable choice of non-linearity, e.g. sigmoid) such that ∀x,∣f(x)−g(x)∣&amp;lt;ϵ. In other words, the &lt;strong&gt;neural network can approximate any continuous function&lt;/strong&gt;&lt;/p&gt;

&lt;h4 id=&quot;setting-the-number-of-layers-in-sizes&quot;&gt;Setting the number of layers in sizes&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;http://cs231n.github.io/assets/nn1/layer_sizes.jpeg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;controlling-the-complexity&quot;&gt;Controlling the complexity&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;http://cs231n.github.io/assets/nn1/reg_strengths.jpeg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

</description>
        <pubDate>Sun, 17 Jul 2016 00:00:00 +0800</pubDate>
        <link>samshaq19912009.github.io/blog/2016/07/17/CNN_3/</link>
        <guid isPermaLink="true">samshaq19912009.github.io/blog/2016/07/17/CNN_3/</guid>
        
        <category>Machine Learning</category>
        
        <category>Notes</category>
        
        <category>CNN</category>
        
        
        <category>blog</category>
        
      </item>
    
      <item>
        <title>CNN 读书笔记（2）</title>
        <description>&lt;h1 id=&quot;linear-classification&quot;&gt;Linear Classification&lt;/h1&gt;

&lt;p&gt;Two disadvantages for KNN classifier:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;remember all the training data and store it for future comparisons&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Classify new test sample is expensive&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;parameterized-mapping-from-images-to-label-scores&quot;&gt;Parameterized mapping from images to label scores&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Score functions&lt;/strong&gt; Maps the raw data to class scores&lt;/p&gt;

&lt;p&gt;Assume traning dataset of images $x_i \in R^D$, each associated with a label $y_i$, here $ i = 1 \dots N$ and $y_i \in { 1 \dots K }$. That is, we have N samples and K distance label features. The &lt;strong&gt;score function&lt;/strong&gt; $f: R^D \mapsto R^K$ that maps the raw image pixels to class scores.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Linear functions&lt;/strong&gt; 
&lt;script type=&quot;math/tex&quot;&gt;f(x_i, W, b) =  W x_i + b&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Interpreting a linear classifier&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://cs231n.github.io/assets/imagemap.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Bias trick&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;$f(x_i, W, b) =  W x_i + b$&lt;/p&gt;

&lt;p&gt;Adding extra dimensions:&lt;/p&gt;

&lt;p&gt;$f(x_i, W) =  W x_i$&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://cs231n.github.io/assets/wb.jpeg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Loss functions&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Quantifies the agreement between the predicted scores and the ground truth labels&lt;/p&gt;

&lt;h2 id=&quot;loss-function&quot;&gt;Loss Function&lt;/h2&gt;

&lt;h3 id=&quot;multiclass-support-vector-machine-loss&quot;&gt;Multiclass Support Vector Machine Loss&lt;/h3&gt;

&lt;p&gt;$L_i = \sum_{j\neq y_i} \max(0, s_j - s_{y_i} + \Delta)
$&lt;/p&gt;

&lt;p&gt;In summary, the SVM loss function wants the score of the correct class $y_i$ to be larger than the incorrect class scores by at least by Δ (delta). If this is not the case, we will accumulate loss.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;The loss function quantifies our unhappiness with predictions on the training set&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In this linear setting&lt;/p&gt;

&lt;p&gt;$L_i = \sum_{j\neq y_i} \max(0, w_j^T x_i - w_{y_i}^T x_i + \Delta)$&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://cs231n.github.io/assets/margin.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;regularization&quot;&gt;Regularization&lt;/h4&gt;

&lt;p&gt;$ R(W) = \sum_k\sum_l W_{k,l}^2$&lt;/p&gt;

&lt;h4 id=&quot;total-loss-functions&quot;&gt;Total loss functions&lt;/h4&gt;

&lt;p&gt;$L = \frac{1}{N} \sum_i \sum_{j\neq y_i} \left[ \max(0, f(x_i; W)&lt;em&gt;j - f(x_i; W)&lt;/em&gt;{y_i} + \Delta) \right] + \lambda \sum_k\sum_l W_{k,l}^2$&lt;/p&gt;

&lt;h2 id=&quot;code-implementation&quot;&gt;Code implementation&lt;/h2&gt;

&lt;h3 id=&quot;svm-naive&quot;&gt;SVM naive&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;def svm_loss_naive(W, X, y, reg):
  &quot;&quot;&quot;
  Structured SVM loss function, naive implementation (with loops).

  Inputs have dimension D, there are C classes, and we operate on minibatches
  of N examples.

  Inputs:
  - W: A numpy array of shape (D, C) containing weights.
  - X: A numpy array of shape (N, D) containing a minibatch of data.
  - y: A numpy array of shape (N,) containing training labels; y[i] = c means
    that X[i] has label c, where 0 &amp;lt;= c &amp;lt; C.
  - reg: (float) regularization strength

  Returns a tuple of:
  - loss as single float
  - gradient with respect to weights W; an array of same shape as W
  &quot;&quot;&quot;
  dW = np.zeros(W.shape) # initialize the gradient as zero

  # compute the loss and the gradient
  num_classes = W.shape[1]
  num_train = X.shape[0]
  loss = 0.0
  for i in xrange(num_train):
    scores = X[i].dot(W) ## C*1 score

    #dS = np.sum(X[i])
    correct_class_score = scores[y[i]]
    diff_count = 0
    for j in xrange(num_classes):

        if j == y[i]:
            continue
        margin = scores[j] - correct_class_score + 1 # note delta = 1
        if margin &amp;gt; 0:
            diff_count += 1 #count for all the incorrect
            dW[:,j] += X[i] #gradient update for incorrect rows
            loss += margin
    dW[:,y[i]] += -diff_count* X[i]#gradient update for correct rows

  # Right now the loss is a sum over all training examples, but we want it
  # to be an average instead so we divide by num_train.
  loss /= num_train
  dW /= num_train
  # Add regularization to the loss.
  loss += 0.5 * reg * np.sum(W * W)
  dW += reg*W

  return loss, dW


&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;svm-vectorized&quot;&gt;SVM Vectorized&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;
def svm_loss_vectorized(W, X, y, reg):
  &quot;&quot;&quot;
  Structured SVM loss function, vectorized implementation.

  Inputs and outputs are the same as svm_loss_naive.
  &quot;&quot;&quot;
  loss = 0.0
  dW = np.zeros(W.shape) # initialize the gradient as zero

  num_train = X.shape[0]

  scores = X.dot(W)
  correct_class_score = scores[np.arange(num_train), y]
  margins = np.maximum(0, scores - correct_class_score[:, np.newaxis] + 1)
  margins[np.arange(num_train), y] = 0
  loss = np.sum(margins)
  loss /= num_train
  loss += 0.5 * reg * reg * np.sum(W*W)


  X_mask = np.zeros(margins.shape)
  X_mask[margins &amp;gt; 0] = 1
  incorrect_counts = np.sum(X_mask, axis=1)
  X_mask[np.arange(num_train),y] = -incorrect_counts
  dW = X.T.dot(X_mask)

  dW /= num_train
  dW += reg * W


  return loss, dW
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;softmax-classifer&quot;&gt;Softmax classifer&lt;/h2&gt;

&lt;p&gt;The Softmax classifier gives a slightly more intuitive output: &lt;strong&gt;normalized class probabilities&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;$L_i = -\log\left(\frac{e^{f_{y_i}}}{ \sum_j e^{f_j} }\right) \hspace{0.5in} \text{or equivalently} \hspace{0.5in} L_i = -f_{y_i} + \log\sum_j e^{f_j}$&lt;/p&gt;

&lt;h4 id=&quot;information-theory-view&quot;&gt;Information theory view&lt;/h4&gt;

&lt;p&gt;The cross-entropy between a “true” distribution p and an estimated distribution q is defined as:&lt;/p&gt;

&lt;p&gt;$H(p,q) = - \sum_x p(x) \log q(x)$&lt;/p&gt;

&lt;p&gt;The Softmax classifier is hence minimizing the cross-entropy between the estimated class probabilities $q = e^{f_{y_i}}  / \sum_j e^{f_j}$as seen above) and the “true” distribution, which in this interpretation is the distribution where all probability mass is on the correct class ($p = [0, \ldots 1, \ldots, 0]$ contains a single 1 at the yi y i-th position.)&lt;/p&gt;

&lt;h4 id=&quot;softmax-naive&quot;&gt;Softmax naive&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;def softmax_loss_naive(W, X, y, reg):
    &quot;&quot;&quot;
  Softmax loss function, naive implementation (with loops)

  Inputs have dimension D, there are C classes, and we operate on minibatches
  of N examples.

  Inputs:
  - W: A numpy array of shape (D, C) containing weights.
  - X: A numpy array of shape (N, D) containing a minibatch of data.
  - y: A numpy array of shape (N,) containing training labels; y[i] = c means
    that X[i] has label c, where 0 &amp;lt;= c &amp;lt; C.
  - reg: (float) regularization strength

  Returns a tuple of:
  - loss as single float
  - gradient with respect to weights W; an array of same shape as W
  &quot;&quot;&quot;
    # Initialize the loss and gradient to zero.
    loss = 0.0
    dW = np.zeros(W.shape)
    # return loss, dW
    # dW = np.zeros(W.shape) # initialize the gradient as zero

    # compute the loss and the gradient
    num_classes = W.shape[1]
    num_train = X.shape[0]
    loss = 0.0
    for i in xrange(num_train):
        scores = X[i].dot(W)  ## C*1 score
        scores -= np.max(scores)

        
        correct_class_score = scores[y[i]]
        
        sum_j = np.sum(np.exp(scores))

        n = np.exp(correct_class_score)
        loss -= np.log(n / sum_j)

        for j in xrange(num_classes):
          if j == y[i]:
            dW[:,j] -= X[i]
          pj = np.exp(scores[j]) / sum_j
          dW[:,j] += pj * X[i]

    loss /= num_train
    dW /= num_train

    # Add regularization to the loss.
    loss += 0.5 * reg * np.sum(W * W)

    dW += reg * W
    return loss, dW

&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&quot;softmax-vectorized&quot;&gt;Softmax Vectorized&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;
def softmax_loss_vectorized(W, X, y, reg):
    &quot;&quot;&quot;
  Softmax loss function, vectorized version.

  Inputs and outputs are the same as softmax_loss_naive.
  &quot;&quot;&quot;
    # Initialize the loss and gradient to zero.

    num_train = X.shape[0]

    dW = np.zeros_like(W)
    scores = X.dot(W)
    scores -= np.max(scores,axis=1)[:,np.newaxis]


    correct_class_score = scores[np.arange(num_train), y]
    divide = np.sum(np.exp(scores),axis=1)
    n = np.exp(correct_class_score)
    error = -1 * np.log( n / divide)
    loss = np.sum(error)


    X_mask = np.exp(scores) / divide[:,np.newaxis]

    #X_mask = np.zeros(error.shape)
    X_mask[np.arange(num_train),y] -= 1

    dW = X.T.dot(X_mask)

    loss /= num_train
    dW /= num_train

    loss += 0.5 * reg * np.sum(W * W)
    dW += reg * W

    return loss, dW
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;stochastic-gradient-decent&quot;&gt;Stochastic gradient Decent&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;class LinearClassifier(object):

  def __init__(self):
    self.W = None

  def train(self, X, y, learning_rate=1e-3, reg=1e-5, num_iters=100,
            batch_size=200, verbose=False):
    &quot;&quot;&quot;
    Train this linear classifier using stochastic gradient descent.

    Inputs:
    - X: A numpy array of shape (N, D) containing training data; there are N
      training samples each of dimension D.
    - y: A numpy array of shape (N,) containing training labels; y[i] = c
      means that X[i] has label 0 &amp;lt;= c &amp;lt; C for C classes.
    - learning_rate: (float) learning rate for optimization.
    - reg: (float) regularization strength.
    - num_iters: (integer) number of steps to take when optimizing
    - batch_size: (integer) number of training examples to use at each step.
    - verbose: (boolean) If true, print progress during optimization.

    Outputs:
    A list containing the value of the loss function at each training iteration.
    &quot;&quot;&quot;
    num_train, dim = X.shape[0], X.shape[1]
    num_classes = np.max(y) + 1 # assume y takes values 0...K-1 where K is number of classes
    if self.W is None:
      # lazily initialize W
      self.W = 0.001 * np.random.randn(dim, num_classes)

    # Run stochastic gradient descent to optimize W
    loss_history = []
    for it in xrange(num_iters):
      X_batch = None
      y_batch = None



      

      # evaluate loss and gradient
      loss, grad = self.loss(X, y, reg)
      loss_history.append(loss)

      self.W +=  - grad * learning_rate


      # perform parameter update


      if verbose and it % 100 == 0:
        print &#39;iteration %d / %d: loss %f&#39; % (it, num_iters, loss)

    return loss_history

  def predict(self, X):
    &quot;&quot;&quot;
    Use the trained weights of this linear classifier to predict labels for
    data points.

    Inputs:
    - X: D x N array of training data. Each column is a D-dimensional point.

    Returns:
    - y_pred: Predicted labels for the data in X. y_pred is a 1-dimensional
      array of length N, and each element is an integer giving the predicted
      class.
    &quot;&quot;&quot;
    y_pred = np.zeros(X.shape[1])

    score = X.dot(self.W)

    y_pred = np.argmax(score,axis=1)

   
    return y_pred

  def loss(self, X_batch, y_batch, reg):
    &quot;&quot;&quot;
    Compute the loss function and its derivative.
    Subclasses will override this.

    Inputs:
    - X_batch: A numpy array of shape (N, D) containing a minibatch of N
      data points; each point has dimension D.
    - y_batch: A numpy array of shape (N,) containing labels for the minibatch.
    - reg: (float) regularization strength.

    Returns: A tuple containing:
    - loss as a single float
    - gradient with respect to self.W; an array of the same shape as W
    &quot;&quot;&quot;
    pass


&lt;/code&gt;&lt;/pre&gt;
</description>
        <pubDate>Sun, 17 Jul 2016 00:00:00 +0800</pubDate>
        <link>samshaq19912009.github.io/blog/2016/07/17/CNN_2/</link>
        <guid isPermaLink="true">samshaq19912009.github.io/blog/2016/07/17/CNN_2/</guid>
        
        <category>Machine Learning</category>
        
        <category>Notes</category>
        
        <category>CNN</category>
        
        <category>SVM</category>
        
        
        <category>blog</category>
        
      </item>
    
  </channel>
</rss>
