<!DOCTYPE html>
<html lang="en">

<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">

    <title>CNN 读书笔记（2） - Chengcen</title>

    <link rel="canonical" href="samshaq19912009.github.io/blog/2016/07/17/CNN_2/">
    <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    TeX: {
      equationNumbers: {
        autoNumber: "AMS"
      }
    },
    tex2jax: {
      inlineMath: [ ['$','$'], ['\(', '\)'] ],
      displayMath: [ ['$$','$$'] ],
      processEscapes: true,
    }
  });
</script>
<script type="text/javascript"
        src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


    <!-- Icons -->
  <link rel="shortcut icon" href="img/favicon.ico">

    <!-- Bootstrap Core CSS -->
    <link rel="stylesheet" href="/css/bootstrap.min.css">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/clean-blog.css">

    <!-- Pygments Github CSS -->
    <link rel="stylesheet" href="/css/syntax.css">

    <!-- Custom Fonts -->
    <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css" rel="stylesheet" type="text/css">
    <link href='//fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
    <link href='//fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800' rel='stylesheet' type='text/css'>

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

</head>






<body ontouchstart="">

    <!-- Navigation -->
<nav class="navbar navbar-default navbar-custom navbar-fixed-top">
    <div class="container-fluid">
        <!-- Brand and toggle get grouped for better mobile display -->
        <div class="navbar-header page-scroll">
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="/">Chengcen</a>
        </div>

        <!-- Collect the nav links, forms, and other content for toggling -->
        <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
            <ul class="nav navbar-nav navbar-right">
                <li>
                    <a href="/">Home</a>
                </li>
                
                <li>
                    <a href="/about/">About</a>
                </li>
                
                <li>
                    <a href="/archive/">Archive</a>
                </li>
                
                <li>
                    <a href="/milestone/">milestone</a>
                </li>
                
                <li>
                    <a href="/tags/">Tags</a>
                </li>
                
            </ul>
        </div>
        <!-- /.navbar-collapse -->
    </div>
    <!-- /.container -->
</nav>


    <!-- Post Header -->
<style type="text/css">
    header.intro-header{
        background-image: url('/img/green.jpg')
    }
</style>
<header class="intro-header" >
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <div class="post-heading">
                    <div class="Tags">
                        
                        <a class="tag" href="/Tags/#Machine Learning" title="Machine Learning">Machine Learning</a>
                        
                        <a class="tag" href="/Tags/#Notes" title="Notes">Notes</a>
                        
                        <a class="tag" href="/Tags/#CNN" title="CNN">CNN</a>
                        
                        <a class="tag" href="/Tags/#SVM" title="SVM">SVM</a>
                        
                    </div>
                    <h1>CNN 读书笔记（2）</h1>
                    
                      
                    <span class="meta">Posted by Chengcen on July 17, 2016</span>
                </div>
            </div>
        </div>
    </div>
</header>




<!-- Post Content -->
<article>
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1 post-container">

                <h1 id="linear-classification">Linear Classification</h1>

<p>Two disadvantages for KNN classifier:</p>

<ul>
  <li>
    <p>remember all the training data and store it for future comparisons</p>
  </li>
  <li>
    <p>Classify new test sample is expensive</p>
  </li>
</ul>

<h2 id="parameterized-mapping-from-images-to-label-scores">Parameterized mapping from images to label scores</h2>

<p><strong>Score functions</strong> Maps the raw data to class scores</p>

<p>Assume traning dataset of images $x_i \in R^D$, each associated with a label $y_i$, here $ i = 1 \dots N$ and $y_i \in { 1 \dots K }$. That is, we have N samples and K distance label features. The <strong>score function</strong> $f: R^D \mapsto R^K$ that maps the raw image pixels to class scores.</p>

<p><strong>Linear functions</strong> 
<script type="math/tex">f(x_i, W, b) =  W x_i + b</script></p>

<p><strong>Interpreting a linear classifier</strong></p>

<p><img src="http://cs231n.github.io/assets/imagemap.jpg" alt="" /></p>

<p><strong>Bias trick</strong></p>

<p>$f(x_i, W, b) =  W x_i + b$</p>

<p>Adding extra dimensions:</p>

<p>$f(x_i, W) =  W x_i$</p>

<p><img src="http://cs231n.github.io/assets/wb.jpeg" alt="" /></p>

<p><strong>Loss functions</strong></p>

<p>Quantifies the agreement between the predicted scores and the ground truth labels</p>

<h2 id="loss-function">Loss Function</h2>

<h3 id="multiclass-support-vector-machine-loss">Multiclass Support Vector Machine Loss</h3>

<p>$L_i = \sum_{j\neq y_i} \max(0, s_j - s_{y_i} + \Delta)
$</p>

<p>In summary, the SVM loss function wants the score of the correct class $y_i$ to be larger than the incorrect class scores by at least by Δ (delta). If this is not the case, we will accumulate loss.</p>

<blockquote>
  <p>The loss function quantifies our unhappiness with predictions on the training set</p>
</blockquote>

<p>In this linear setting</p>

<p>$L_i = \sum_{j\neq y_i} \max(0, w_j^T x_i - w_{y_i}^T x_i + \Delta)$</p>

<p><img src="http://cs231n.github.io/assets/margin.jpg" alt="" /></p>

<h4 id="regularization">Regularization</h4>

<p>$ R(W) = \sum_k\sum_l W_{k,l}^2$</p>

<h4 id="total-loss-functions">Total loss functions</h4>

<p>$L = \frac{1}{N} \sum_i \sum_{j\neq y_i} \left[ \max(0, f(x_i; W)<em>j - f(x_i; W)</em>{y_i} + \Delta) \right] + \lambda \sum_k\sum_l W_{k,l}^2$</p>

<h2 id="code-implementation">Code implementation</h2>

<h3 id="svm-naive">SVM naive</h3>

<pre><code class="language-Python">def svm_loss_naive(W, X, y, reg):
  """
  Structured SVM loss function, naive implementation (with loops).

  Inputs have dimension D, there are C classes, and we operate on minibatches
  of N examples.

  Inputs:
  - W: A numpy array of shape (D, C) containing weights.
  - X: A numpy array of shape (N, D) containing a minibatch of data.
  - y: A numpy array of shape (N,) containing training labels; y[i] = c means
    that X[i] has label c, where 0 &lt;= c &lt; C.
  - reg: (float) regularization strength

  Returns a tuple of:
  - loss as single float
  - gradient with respect to weights W; an array of same shape as W
  """
  dW = np.zeros(W.shape) # initialize the gradient as zero

  # compute the loss and the gradient
  num_classes = W.shape[1]
  num_train = X.shape[0]
  loss = 0.0
  for i in xrange(num_train):
    scores = X[i].dot(W) ## C*1 score

    #dS = np.sum(X[i])
    correct_class_score = scores[y[i]]
    diff_count = 0
    for j in xrange(num_classes):

        if j == y[i]:
            continue
        margin = scores[j] - correct_class_score + 1 # note delta = 1
        if margin &gt; 0:
            diff_count += 1 #count for all the incorrect
            dW[:,j] += X[i] #gradient update for incorrect rows
            loss += margin
    dW[:,y[i]] += -diff_count* X[i]#gradient update for correct rows

  # Right now the loss is a sum over all training examples, but we want it
  # to be an average instead so we divide by num_train.
  loss /= num_train
  dW /= num_train
  # Add regularization to the loss.
  loss += 0.5 * reg * np.sum(W * W)
  dW += reg*W

  return loss, dW


</code></pre>

<h3 id="svm-vectorized">SVM Vectorized</h3>

<pre><code class="language-Python">
def svm_loss_vectorized(W, X, y, reg):
  """
  Structured SVM loss function, vectorized implementation.

  Inputs and outputs are the same as svm_loss_naive.
  """
  loss = 0.0
  dW = np.zeros(W.shape) # initialize the gradient as zero

  num_train = X.shape[0]

  scores = X.dot(W)
  correct_class_score = scores[np.arange(num_train), y]
  margins = np.maximum(0, scores - correct_class_score[:, np.newaxis] + 1)
  margins[np.arange(num_train), y] = 0
  loss = np.sum(margins)
  loss /= num_train
  loss += 0.5 * reg * reg * np.sum(W*W)


  X_mask = np.zeros(margins.shape)
  X_mask[margins &gt; 0] = 1
  incorrect_counts = np.sum(X_mask, axis=1)
  X_mask[np.arange(num_train),y] = -incorrect_counts
  dW = X.T.dot(X_mask)

  dW /= num_train
  dW += reg * W


  return loss, dW
</code></pre>

<h2 id="softmax-classifer">Softmax classifer</h2>

<p>The Softmax classifier gives a slightly more intuitive output: <strong>normalized class probabilities</strong></p>

<p>$L_i = -\log\left(\frac{e^{f_{y_i}}}{ \sum_j e^{f_j} }\right) \hspace{0.5in} \text{or equivalently} \hspace{0.5in} L_i = -f_{y_i} + \log\sum_j e^{f_j}$</p>

<h4 id="information-theory-view">Information theory view</h4>

<p>The cross-entropy between a “true” distribution p and an estimated distribution q is defined as:</p>

<p>$H(p,q) = - \sum_x p(x) \log q(x)$</p>

<p>The Softmax classifier is hence minimizing the cross-entropy between the estimated class probabilities $q = e^{f_{y_i}}  / \sum_j e^{f_j}$as seen above) and the “true” distribution, which in this interpretation is the distribution where all probability mass is on the correct class ($p = [0, \ldots 1, \ldots, 0]$ contains a single 1 at the yi y i-th position.)</p>

<h4 id="softmax-naive">Softmax naive</h4>

<pre><code class="language-Python">def softmax_loss_naive(W, X, y, reg):
    """
  Softmax loss function, naive implementation (with loops)

  Inputs have dimension D, there are C classes, and we operate on minibatches
  of N examples.

  Inputs:
  - W: A numpy array of shape (D, C) containing weights.
  - X: A numpy array of shape (N, D) containing a minibatch of data.
  - y: A numpy array of shape (N,) containing training labels; y[i] = c means
    that X[i] has label c, where 0 &lt;= c &lt; C.
  - reg: (float) regularization strength

  Returns a tuple of:
  - loss as single float
  - gradient with respect to weights W; an array of same shape as W
  """
    # Initialize the loss and gradient to zero.
    loss = 0.0
    dW = np.zeros(W.shape)
    # return loss, dW
    # dW = np.zeros(W.shape) # initialize the gradient as zero

    # compute the loss and the gradient
    num_classes = W.shape[1]
    num_train = X.shape[0]
    loss = 0.0
    for i in xrange(num_train):
        scores = X[i].dot(W)  ## C*1 score
        scores -= np.max(scores)

        
        correct_class_score = scores[y[i]]
        
        sum_j = np.sum(np.exp(scores))

        n = np.exp(correct_class_score)
        loss -= np.log(n / sum_j)

        for j in xrange(num_classes):
          if j == y[i]:
            dW[:,j] -= X[i]
          pj = np.exp(scores[j]) / sum_j
          dW[:,j] += pj * X[i]

    loss /= num_train
    dW /= num_train

    # Add regularization to the loss.
    loss += 0.5 * reg * np.sum(W * W)

    dW += reg * W
    return loss, dW

</code></pre>

<h4 id="softmax-vectorized">Softmax Vectorized</h4>

<pre><code class="language-Python">
def softmax_loss_vectorized(W, X, y, reg):
    """
  Softmax loss function, vectorized version.

  Inputs and outputs are the same as softmax_loss_naive.
  """
    # Initialize the loss and gradient to zero.

    num_train = X.shape[0]

    dW = np.zeros_like(W)
    scores = X.dot(W)
    scores -= np.max(scores,axis=1)[:,np.newaxis]


    correct_class_score = scores[np.arange(num_train), y]
    divide = np.sum(np.exp(scores),axis=1)
    n = np.exp(correct_class_score)
    error = -1 * np.log( n / divide)
    loss = np.sum(error)


    X_mask = np.exp(scores) / divide[:,np.newaxis]

    #X_mask = np.zeros(error.shape)
    X_mask[np.arange(num_train),y] -= 1

    dW = X.T.dot(X_mask)

    loss /= num_train
    dW /= num_train

    loss += 0.5 * reg * np.sum(W * W)
    dW += reg * W

    return loss, dW
</code></pre>

<h3 id="stochastic-gradient-decent">Stochastic gradient Decent</h3>

<pre><code class="language-Python">class LinearClassifier(object):

  def __init__(self):
    self.W = None

  def train(self, X, y, learning_rate=1e-3, reg=1e-5, num_iters=100,
            batch_size=200, verbose=False):
    """
    Train this linear classifier using stochastic gradient descent.

    Inputs:
    - X: A numpy array of shape (N, D) containing training data; there are N
      training samples each of dimension D.
    - y: A numpy array of shape (N,) containing training labels; y[i] = c
      means that X[i] has label 0 &lt;= c &lt; C for C classes.
    - learning_rate: (float) learning rate for optimization.
    - reg: (float) regularization strength.
    - num_iters: (integer) number of steps to take when optimizing
    - batch_size: (integer) number of training examples to use at each step.
    - verbose: (boolean) If true, print progress during optimization.

    Outputs:
    A list containing the value of the loss function at each training iteration.
    """
    num_train, dim = X.shape[0], X.shape[1]
    num_classes = np.max(y) + 1 # assume y takes values 0...K-1 where K is number of classes
    if self.W is None:
      # lazily initialize W
      self.W = 0.001 * np.random.randn(dim, num_classes)

    # Run stochastic gradient descent to optimize W
    loss_history = []
    for it in xrange(num_iters):
      X_batch = None
      y_batch = None



      

      # evaluate loss and gradient
      loss, grad = self.loss(X, y, reg)
      loss_history.append(loss)

      self.W +=  - grad * learning_rate


      # perform parameter update


      if verbose and it % 100 == 0:
        print 'iteration %d / %d: loss %f' % (it, num_iters, loss)

    return loss_history

  def predict(self, X):
    """
    Use the trained weights of this linear classifier to predict labels for
    data points.

    Inputs:
    - X: D x N array of training data. Each column is a D-dimensional point.

    Returns:
    - y_pred: Predicted labels for the data in X. y_pred is a 1-dimensional
      array of length N, and each element is an integer giving the predicted
      class.
    """
    y_pred = np.zeros(X.shape[1])

    score = X.dot(self.W)

    y_pred = np.argmax(score,axis=1)

   
    return y_pred

  def loss(self, X_batch, y_batch, reg):
    """
    Compute the loss function and its derivative.
    Subclasses will override this.

    Inputs:
    - X_batch: A numpy array of shape (N, D) containing a minibatch of N
      data points; each point has dimension D.
    - y_batch: A numpy array of shape (N,) containing labels for the minibatch.
    - reg: (float) regularization strength.

    Returns: A tuple containing:
    - loss as a single float
    - gradient with respect to self.W; an array of the same shape as W
    """
    pass


</code></pre>


                <hr>

                <ul class="pager">
                    
                    <li class="previous">
                        <a href="/blog/2016/07/17/CNN_part_1/" data-toggle="tooltip" data-placement="top" title="CNN 读书笔记（1）">&larr; Previous Post</a>
                    </li>
                    
                    
                    <li class="next">
                        <a href="/blog/2016/07/17/CNN_3/" data-toggle="tooltip" data-placement="top" title="CNN 读书笔记（3） Netural Network 初入门">Next Post &rarr;</a>
                    </li>
                    
                </ul>

                <!-- Duoshuo Share start -->
                <style>
                    .ds-share{
                        text-align: right;
                    }
                    
                    @media only screen and (max-width: 700px) {
                        .ds-share {

                        }
                    }
                </style>

                <div class="ds-share"
                    data-thread-key="/blog/2016/07/17/CNN_2" data-title="CNN 读书笔记（2）"
                    data-images="samshaq19912009.github.io/img/green.jpg"
                    data-content="Linear Classification

Two disadvantages for KNN classifier:


  
    remembe... | Microdust:Azeril's blog"
                    data-url="samshaq19912009.github.io/blog/2016/07/17/CNN_2/">
                    <div class="ds-share-inline">
                      <ul  class="ds-share-icons-16">

                        <li data-toggle="ds-share-icons-more"><a class="ds-more" href="#">分享到：</a></li>
                        <li><a class="ds-wechat flat" href="javascript:void(0);" data-service="wechat">微信</a></li>
                        <li><a class="ds-weibo flat" href="javascript:void(0);" data-service="weibo">微博</a></li>
                        <li><a class="ds-douban flat" href="javascript:void(0);" data-service="douban">豆瓣</a></li>
                      </ul>
                      <div class="ds-share-icons-more">
                      </div>
                    </div>
                <hr>
                </div>
                <!-- Duoshuo Share end-->


                <!-- 多说评论框 start -->
                <div class="comment">
                    <div class="ds-thread" data-thread-key="/blog/2016/07/17/CNN_2" data-title="CNN 读书笔记（2）" data-url="samshaq19912009.github.io/blog/2016/07/17/CNN_2/"></div>
                </div>
                <!-- 多说评论框 end -->
            </div>
        </div>
    </div>
</article>



<!-- 多说公共JS代码 start (一个网页只需插入一次) -->
<script type="text/javascript">
var duoshuoQuery = {short_name:"cnfeat"};
    (function() {
        var ds = document.createElement('script');
        ds.type = 'text/javascript';ds.async = true;
        ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
        ds.charset = 'UTF-8';
        (document.getElementsByTagName('head')[0]
         || document.getElementsByTagName('body')[0]).appendChild(ds);
    })();
</script>
<!-- 多说公共JS代码 end -->

<!-- async load function -->
<script>
    function async(u, c) {
      var d = document, t = 'script',
          o = d.createElement(t),
          s = d.getElementsByTagName(t)[0];
      o.src = u;
      if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
      s.parentNode.insertBefore(o, s);
    }
</script>

<!-- anchor-js, Doc:http://bryanbraun.github.io/anchorjs/ -->

<style>
    /* place left on bigger screen */
    @media all and (min-width: 800px) {
        .anchorjs-link{
            position: absolute;
            left: -0.75em;
            font-size: 1.1em;
            margin-top : -0.1em;
        }
    }
</style>


    <!-- Footer -->
<footer>
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <ul class="list-inline text-center">
                    <!-- kill the Facebook and Weibo -->
                    <li>
                        <a href="/feed.xml">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-rss fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                    

                    
                    
                    <!--
                    
                    -->

                    <!--
                    
                    -->

                    
                    <!--
                    
                    -->

                </ul>
                <p class="copyright text-muted">
                &copy; 2016  Chengcen  ❖ Powered by Jekyll.
                </p>
            </div>
        </div>
    </div>
</footer>

<!-- jQuery -->
<script src="/js/jquery.min.js "></script>

<!-- Bootstrap Core JavaScript -->
<script src="/js/bootstrap.min.js "></script>

<!-- Custom Theme JavaScript -->
<script src="/js/clean-blog.min.js "></script>



<!-- Highlight.js -->
<script>
    async("http://cdn.bootcss.com/highlight.js/8.6/highlight.min.js",function(){
        hljs.initHighlightingOnLoad();
    })
</script>
<link href="http://cdn.bootcss.com/highlight.js/8.6/styles/github.min.css" rel="stylesheet">


</body>

</html>