<!DOCTYPE html>
<html lang="en">

<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">

    <title>NLP tutorial 电影评论情感分析 - Chengcen</title>

    <link rel="canonical" href="samshaq19912009.github.io/blog/2016/06/20/NLP_tutorial/">
    <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    TeX: {
      equationNumbers: {
        autoNumber: "AMS"
      }
    },
    tex2jax: {
      inlineMath: [ ['$','$'], ['\(', '\)'] ],
      displayMath: [ ['$$','$$'] ],
      processEscapes: true,
    }
  });
</script>
<script type="text/javascript"
        src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


    <!-- Icons -->
  <link rel="shortcut icon" href="img/favicon.ico">

    <!-- Bootstrap Core CSS -->
    <link rel="stylesheet" href="/css/bootstrap.min.css">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/clean-blog.css">

    <!-- Pygments Github CSS -->
    <link rel="stylesheet" href="/css/syntax.css">

    <!-- Custom Fonts -->
    <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css" rel="stylesheet" type="text/css">
    <link href='//fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
    <link href='//fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800' rel='stylesheet' type='text/css'>

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

</head>






<body ontouchstart="">

    <!-- Navigation -->
<nav class="navbar navbar-default navbar-custom navbar-fixed-top">
    <div class="container-fluid">
        <!-- Brand and toggle get grouped for better mobile display -->
        <div class="navbar-header page-scroll">
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="/">Chengcen</a>
        </div>

        <!-- Collect the nav links, forms, and other content for toggling -->
        <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
            <ul class="nav navbar-nav navbar-right">
                <li>
                    <a href="/">Home</a>
                </li>
                
                <li>
                    <a href="/about/">About</a>
                </li>
                
                <li>
                    <a href="/archive/">Archive</a>
                </li>
                
                <li>
                    <a href="/milestone/">milestone</a>
                </li>
                
                <li>
                    <a href="/tags/">Tags</a>
                </li>
                
            </ul>
        </div>
        <!-- /.navbar-collapse -->
    </div>
    <!-- /.container -->
</nav>


    <!-- Post Header -->
<style type="text/css">
    header.intro-header{
        background-image: url('/img/green.jpg')
    }
</style>
<header class="intro-header" >
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <div class="post-heading">
                    <div class="Tags">
                        
                        <a class="tag" href="/Tags/#Machine Learning" title="Machine Learning">Machine Learning</a>
                        
                        <a class="tag" href="/Tags/#Notes" title="Notes">Notes</a>
                        
                    </div>
                    <h1>NLP tutorial 电影评论情感分析</h1>
                    
                      
                    <span class="meta">Posted by Chengcen on June 20, 2016</span>
                </div>
            </div>
        </div>
    </div>
</header>




<!-- Post Content -->
<article>
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1 post-container">

                <h1 id="nlp-tutorial-">NLP tutorial: 电影评论情感分析</h1>

<p>自然语言处理是用来处理语言文字的工具，目的是为了把自然語言轉化為計算機程序更易于處理的形式。这一篇tutorial是用来分析电影评论的情感（sentimental anaylysis）。</p>

<h2 id="section">读入数据</h2>

<p>首先需要录入data， train data有三行，id, sentiment(0 or 1)和reviews, 目的是根据review来预测sentiment。</p>

<h3 id="section-1">数据清理</h3>

<p>根据每一行的review，可以发现里面存在很多HTML的tag，这些tag需要去除。这里可以用BeautifulSoup来清理</p>

<pre><code class="language-Python"># Import BeautifulSoup into your workspace
from bs4 import BeautifulSoup             

# Initialize the BeautifulSoup object on a single movie review     
example1 = BeautifulSoup(train["review"][0])  

# Print the raw review and then the output of get_text(), for
# comparison
print train["review"][0]
print example1.get_text()
</code></pre>

<p>这样可以得到纯文字的文本。进行下一步的处理</p>

<h3 id="section-2">处理数字，标点</h3>

<p>在很多自然语言分析中，数字和标点符号可以有不同的含义。例如，感叹号可以在分析情感中起到作用，这里出于简化，我们把非字母的字符都去掉，可以使用正则表达式（regular expression)</p>

<pre><code class="language-Python">import re
# Use regular expressions to do a find-and-replace
letters_only = re.sub("[^a-zA-Z]",           # The pattern to search for
                      " ",                   # The pattern to replace it with
                      example1.get_text() )  # The text to search
print letters_only

</code></pre>
<p>最后，我们再加入stop words，一些常见的单词不计入最后的数据</p>

<pre><code class="language-Python"># Remove stop words from "words"
words = [w for w in words if not w in stopwords.words("english")]
print words
</code></pre>

<h2 id="bag-of-words">Bag of words</h2>

<p>Bag of words model:
从所有的文档出提取出一个词汇表（vocabulary）， 然后对每一个文档统计每一个单词出现的频率。例如以下两个句子：</p>

<p>Sentence 1: “The cat sat on the hat”</p>

<p>Sentence 2: “The dog ate the cat and the hat”</p>

<p>根据以上两个句子，所得的vocabulary为：</p>

<p>{ the, cat, sat, on, hat, dog, ate, and }</p>

<p>这样， 每一个句子就转换为一个向量，</p>

<p>Sentence 1: { 2, 1, 1, 1, 1, 0, 0, 0 }</p>

<p>Sentence 2：{ 3, 1, 0, 0, 1, 1, 1, 1}</p>

<p>在这里处理电影评论的时候，由于总体vocabulary量过大，只选用最长出现的500个单词，利用scikit-learn的 <a href="http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html">CountVectorizer</a>.具体代码如下：</p>

<pre><code class="language-Python">print "Creating the bag of words...\n"
from sklearn.feature_extraction.text import CountVectorizer

# Initialize the "CountVectorizer" object, which is scikit-learn's
# bag of words tool.  
vectorizer = CountVectorizer(analyzer = "word",   \
                             tokenizer = None,    \
                             preprocessor = None, \
                             stop_words = None,   \
                             max_features = 5000)

# fit_transform() does two functions: First, it fits the model
# and learns the vocabulary; second, it transforms our training data
# into feature vectors. The input to fit_transform should be a list of
# strings.
train_data_features = vectorizer.fit_transform(clean_train_reviews)

# Numpy arrays are easy to work with, so convert the result to an
# array
train_data_features = train_data_features.toarray()

</code></pre>
<p>如果想要得到vocabulary的具体feature，或者分析某一个Word在所有文档里出现的频率，可以使用如下代码</p>

<pre><code class="language-Python"># Take a look at the words in the vocabulary
vocab = vectorizer.get_feature_names()
print vocab

# Sum up the counts of each vocabulary word
dist = np.sum(train_data_features, axis=0)

# For each, print the vocabulary word and the number of times it
# appears in the training set
for tag, count in zip(vocab, dist):
    print count, tag

</code></pre>

<h2 id="random-forest">Random Forest</h2>

<p>现在对于每一个review， 已经转换成了一个向量，接下来，使用<a href="http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html">Random Forest</a>的方法来进行分类。</p>

<pre><code class="language-Python">print "Training the random forest..."
from sklearn.ensemble import RandomForestClassifier

# Initialize a Random Forest classifier with 100 trees
forest = RandomForestClassifier(n_estimators = 100)

# Fit the forest to the training set, using the bag of words as
# features and the sentiment labels as the response variable
#
# This may take a few minutes to run
forest = forest.fit( train_data_features, train["sentiment"] )

</code></pre>

<p>训练好model后，就可以录入test data，重复之前我们对train data的pre-processing，再predict label</p>

<pre><code class="language-Python"># Read the test data
test = pd.read_csv("testData.tsv", header=0, delimiter="\t", \
                   quoting=3 )

# Verify that there are 25,000 rows and 2 columns
print test.shape

# Create an empty list and append the clean reviews one by one
num_reviews = len(test["review"])
clean_test_reviews = []

print "Cleaning and parsing the test set movie reviews...\n"
for i in xrange(0,num_reviews):
    if( (i+1) % 1000 == 0 ):
        print "Review %d of %d\n" % (i+1, num_reviews)
    clean_review = review_to_words( test["review"][i] )
    clean_test_reviews.append( clean_review )

# Get a bag of words for the test set, and convert to a numpy array
test_data_features = vectorizer.transform(clean_test_reviews)
test_data_features = test_data_features.toarray()

# Use the random forest to make sentiment label predictions
result = forest.predict(test_data_features)

# Copy the results to a pandas dataframe with an "id" column and
# a "sentiment" column
output = pd.DataFrame( data={"id":test["id"], "sentiment":result} )

# Use pandas to write the comma-separated output file
output.to_csv( "Bag_of_Words_model.csv", index=False, quoting=3 )

</code></pre>

<h2 id="word-vector-model">Word Vector Model</h2>

<p>接下来使用 word vector model,将每一个Word转换成一个vector，数据清理阶段与上一个model基本相同。</p>

<p>注意的是，这时候需要的是sentence！！！！所以不能直接把某一段paragraph直接换成Wordlist！！！！！</p>

<p>每一个review是一段paragraph，先把paragraph转换成每一个句子，使用<strong>tokenizer.tokenize</strong></p>

<pre><code class="language-Python">

# Define a function to split a review into parsed sentences
def review_to_sentences( review, tokenizer, remove_stopwords=False ):
    # Function to split a review into parsed sentences. Returns a
    # list of sentences, where each sentence is a list of words
    #
    # 1. Use the NLTK tokenizer to split the paragraph into sentences

    raw_sentences = tokenizer.tokenize(unicode(review.strip(),errors='ignore'))##convert paragraph into sentences

    #
    # 2. Loop over each sentence
    sentences = []
    for raw_sentence in raw_sentences:
        # If a sentence is empty, skip it
        if len(raw_sentence) &gt; 0:
            # Otherwise, call review_to_wordlist to get a list of words
            sentences.append( review_to_wordlist( raw_sentence, \
              remove_stopwords ))
    #
    # Return the list of sentences (each sentence is a list of words,
    # so this returns a list of lists
    return sentences



</code></pre>

<h3 id="training-and-saving">Training and Saving</h3>

<p>使用Python的genism package进行word2Vec训练，基本参数为</p>

<blockquote>
  <p><strong>Context / window size</strong> 每次训练时的context size？</p>

  <p><strong>Word vector dimensionality</strong> word vector的长度</p>

  <p><strong>Downsampling of frequent words</strong>：对常见Word的downsampling，提高模型准确率</p>

  <p><strong>Minimum word count</strong>：减少最终vocabulary的Word数目</p>
</blockquote>

<pre><code class="language-Python">
# Import the built-in logging module and configure it so that Word2Vec
# creates nice output messages
import logging
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\
    level=logging.INFO)

# Set values for various parameters
num_features = 300    # Word vector dimensionality                      
min_word_count = 40   # Minimum word count                        
num_workers = 4       # Number of threads to run in parallel
context = 10          # Context window size                                                                                    
downsampling = 1e-3   # Downsample setting for frequent words

# Initialize and train the model (this will take some time)
from gensim.models import word2vec
print "Training model..."
model = word2vec.Word2Vec(sentences, workers=num_workers, \
            size=num_features, min_count = min_word_count, \
            window = context, sample = downsampling)

# If you don't plan to train the model any further, calling
# init_sims will make the model much more memory-efficient.
model.init_sims(replace=True)

# It can be helpful to create a meaningful model name and
# save the model for later use. You can load it later using Word2Vec.load()
model_name = "300features_40minwords_10context"
model.save(model_name)

</code></pre>

<p>模型训练完毕后，可以用模型来预测，模拟。例子如下</p>

<pre><code class="language-Python">#哪一个单词和以下最不接近？

model.doesnt_match("man woman child kitchen".split())

#输出‘kitchen’


model.doesnt_match("france england germany berlin".split())

#输出‘Berlin’

#哪些单词最接近？
model.most_similar("man")

#输出
[(u'woman', 0.6092387437820435),
 (u'lad', 0.6064352989196777),
 (u'lady', 0.57511305809021),
 (u'monk', 0.5583359599113464),
 (u'sailor', 0.534398078918457),
 (u'men', 0.5196300745010376),
 (u'farmer', 0.5171995162963867),
 (u'person', 0.5137056112289429),
 (u'guy', 0.501788854598999),
 (u'millionaire', 0.4980509281158447)]
</code></pre>
<p>此外，可以看到，某一个单词的vector length都是300.</p>

<pre><code class="language-Python">len(model['computer'])

#输出300

</code></pre>

<h2 id="more-fun-with-word-vector">More fun with Word Vector</h2>

<h3 id="from-words-to-paragraphs-attempt-1-vector-averaging">From Words To Paragraphs, Attempt 1: Vector Averaging</h3>

<p>对于每一篇review， 长度都不固定，所以对于每一篇review，需要将每一个Word vector转换成段落的feature。</p>

<p>给定一段review，计算平均的Wordvector， 以下code中， mode.index2word输出原始的Word dictionary。</p>

<pre><code class="language-Python">def makeFeatureVec(words, model, num_features):
    # Function to average all of the word vectors in a given
    # paragraph
    #
    # Pre-initialize an empty numpy array (for speed)
    featureVec = np.zeros((num_features,),dtype="float32")
    #
    nwords = 0.
    #
    # Index2word is a list that contains the names of the words in
    # the model's vocabulary. Convert it to a set, for speed
    index2word_set = set(model.index2word)
    #
    # Loop over each word in the review and, if it is in the model's
    # vocaublary, add its feature vector to the total
    for word in words:
        if word in index2word_set:
            nwords = nwords + 1.
            featureVec = np.add(featureVec,model[word])
    #
    # Divide the result by the number of words to get the average
    featureVec = np.divide(featureVec,nwords)
    return featureVec
</code></pre>

<p>在这段code的基础上，给定很多review，输出平均的Word vector。</p>

<pre><code class="language-Python">def getAvgFeatureVecs(reviews, model, num_features):
    # Given a set of reviews (each one a list of words), calculate
    # the average feature vector for each one and return a 2D numpy array
    #
    # Initialize a counter
    counter = 0.
    #
    # Preallocate a 2D numpy array, for speed
    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype="float32")
    #
    # Loop through the reviews
    for review in reviews:
       #
       # Print a status message every 1000th review
       if counter%1000. == 0.:
           print "Review %d of %d" % (counter, len(reviews))
       #
       # Call the function (defined above) that makes average feature vectors
       reviewFeatureVecs[counter] = makeFeatureVec(review, model, \
           num_features)
       #
       # Increment the counter
       counter = counter + 1.
    return reviewFeatureVecs


</code></pre>

<p>根据以上的代码，可以计算把train data 和 test data转换成clean的review</p>

<pre><code class="language-Python"># ****************************************************************
# Calculate average feature vectors for training and testing sets,
# using the functions we defined above. Notice that we now use stop word
# removal.

clean_train_reviews = []
for review in train["review"]:
    clean_train_reviews.append( review_to_wordlist( review, \
        remove_stopwords=True ))

trainDataVecs = getAvgFeatureVecs( clean_train_reviews, model, num_features )

print "Creating average feature vecs for test reviews"
clean_test_reviews = []
for review in test["review"]:
    clean_test_reviews.append( review_to_wordlist( review, \
        remove_stopwords=True ))

testDataVecs = getAvgFeatureVecs( clean_test_reviews, model, num_features )

</code></pre>

<p>让我们来检查一下最后得到的trainDataVecs和testDataVects的dimension。可以看到trainDataVecs的dimension为（25000， 300），意为我们有25000个training example，每个example为一个300维的word vector；同理, testDataVecs的dimension也为(25000， 300）。</p>

<p>和bag of word一样，我们可以采用random forest的方法去训练model，这里不再重复。</p>

<h3 id="from-words-to-paragraphs-attempt-2-clustering">From Words To Paragraphs, Attempt 2: Clustering</h3>

<p>Word2Vec将会输出相似语义词汇的cluster， 所以，另一个常用方法就是利用同一cluster之间词汇的相似性。 将向量按此分类叫做<strong>Vector quantization</strong>.首先，利用<a href="http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html">K-means algorithm</a>来确定每一个cluster的center。</p>

<pre><code class="language-Python">from sklearn.cluster import KMeans
import time

start = time.time() # Start time

# Set "k" (num_clusters) to be 1/5th of the vocabulary size, or an
# average of 5 words per cluster
word_vectors = model.syn0
num_clusters = word_vectors.shape[0] / 5

# Initalize a k-means object and use it to extract centroids
kmeans_clustering = KMeans( n_clusters = num_clusters )
idx = kmeans_clustering.fit_predict( word_vectors )

# Get the end time and print how long the process took
end = time.time()
elapsed = end - start
print "Time taken for K Means clustering: ", elapsed, "seconds."

</code></pre>

<p>由于cluster的数目过多，以上code需要很长时间来进行训练（在我的电脑上花了24分钟！），idx会是每一个word_vector对应的cluster number。定义一个dictionary，把每一个word map到index number上。</p>

<pre><code class="language-Python"># Create a Word / Index dictionary, mapping each vocabulary word to
# a cluster number                                                                                            
word_centroid_map = dict(zip( model.index2word, idx ))

</code></pre>
<p>根据以上的dictionary，我们可以把每一条review转换成bag of centroids.</p>

<pre><code class="language-Python">def create_bag_of_centroids( wordlist, word_centroid_map ):
    #
    # The number of clusters is equal to the highest cluster index
    # in the word / centroid map
    num_centroids = max( word_centroid_map.values() ) + 1
    #
    # Pre-allocate the bag of centroids vector (for speed)
    bag_of_centroids = np.zeros( num_centroids, dtype="float32" )
    #
    # Loop over the words in the review. If the word is in the vocabulary,
    # find which cluster it belongs to, and increment that cluster count
    # by one
    for word in wordlist:
        if word in word_centroid_map:
            index = word_centroid_map[word]
            bag_of_centroids[index] += 1
    #
    # Return the "bag of centroids"
    return bag_of_centroids

</code></pre>
<p>具体分析为，现在number of cluster为3295，对每一条review，转换为3295维的向量，此向量的i位置对应为，出现在第i个cluster的word的数目。</p>

<p>对每一条train和test的review，我们都可以做次变化，得到train_centroids和test_centroids。</p>

<pre><code class="language-Python"># Pre-allocate an array for the training set bags of centroids (for speed)
train_centroids = np.zeros( (train["review"].size, num_clusters), \
    dtype="float32" )

# Transform the training set reviews into bags of centroids
counter = 0
for review in clean_train_reviews:
    train_centroids[counter] = create_bag_of_centroids( review, \
        word_centroid_map )
    counter += 1

# Repeat for test reviews
test_centroids = np.zeros(( test["review"].size, num_clusters), \
    dtype="float32" )

counter = 0
for review in clean_test_reviews:
    test_centroids[counter] = create_bag_of_centroids( review, \
        word_centroid_map )
    counter += 1

</code></pre>

<p>我们来检查一下train_centroids.shape，结果为(25000, 3295)，意为有25000个train sample， 每个sample为3295维的向量。比较之前的train sample维度为(25000, 300)。显然，cluster的方法保留了更多原有信息，也使得模型变得更加复杂。</p>

<p>同理，我们可以用Random Forest进行训练，与之前的code比较，此结果略差于bag of words model。</p>

<h2 id="conclusion">Conclusion</h2>

<p>为什么利用word2vec得到的结果与之前的bag of words模型类似？最大的原因在于，利用average或者k-means methods，我们已经忽略了word的<strong>order</strong>， 这样，这与bag of words的模型假设已经非常类似。</p>

<p>一些提高的建议：</p>

<p>1.使用更多的text来训练word2vec model。
2.使用<strong>Paragraph Vector</strong></p>


                <hr>

                <ul class="pager">
                    
                    <li class="previous">
                        <a href="/blog/2016/06/16/FP_Tree/" data-toggle="tooltip" data-placement="top" title="FP-growth Algorithm">&larr; Previous Post</a>
                    </li>
                    
                    
                    <li class="next">
                        <a href="/blog/2016/06/23/LDA/" data-toggle="tooltip" data-placement="top" title="LDA 简介">Next Post &rarr;</a>
                    </li>
                    
                </ul>

                <!-- Duoshuo Share start -->
                <style>
                    .ds-share{
                        text-align: right;
                    }
                    
                    @media only screen and (max-width: 700px) {
                        .ds-share {

                        }
                    }
                </style>

                <div class="ds-share"
                    data-thread-key="/blog/2016/06/20/NLP_tutorial" data-title="NLP tutorial 电影评论情感分析"
                    data-images="samshaq19912009.github.io/img/green.jpg"
                    data-content="NLP tutorial: 电影评论情感分析

自然语言处理是用来处理语言文字的工具，目的是为了把自然語言轉化為計算機程序更易于處理的形式。这一篇tuto... | Microdust:Azeril's blog"
                    data-url="samshaq19912009.github.io/blog/2016/06/20/NLP_tutorial/">
                    <div class="ds-share-inline">
                      <ul  class="ds-share-icons-16">

                        <li data-toggle="ds-share-icons-more"><a class="ds-more" href="#">分享到：</a></li>
                        <li><a class="ds-wechat flat" href="javascript:void(0);" data-service="wechat">微信</a></li>
                        <li><a class="ds-weibo flat" href="javascript:void(0);" data-service="weibo">微博</a></li>
                        <li><a class="ds-douban flat" href="javascript:void(0);" data-service="douban">豆瓣</a></li>
                      </ul>
                      <div class="ds-share-icons-more">
                      </div>
                    </div>
                <hr>
                </div>
                <!-- Duoshuo Share end-->


                <!-- 多说评论框 start -->
                <div class="comment">
                    <div class="ds-thread" data-thread-key="/blog/2016/06/20/NLP_tutorial" data-title="NLP tutorial 电影评论情感分析" data-url="samshaq19912009.github.io/blog/2016/06/20/NLP_tutorial/"></div>
                </div>
                <!-- 多说评论框 end -->
            </div>
        </div>
    </div>
</article>



<!-- 多说公共JS代码 start (一个网页只需插入一次) -->
<script type="text/javascript">
var duoshuoQuery = {short_name:"cnfeat"};
    (function() {
        var ds = document.createElement('script');
        ds.type = 'text/javascript';ds.async = true;
        ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
        ds.charset = 'UTF-8';
        (document.getElementsByTagName('head')[0]
         || document.getElementsByTagName('body')[0]).appendChild(ds);
    })();
</script>
<!-- 多说公共JS代码 end -->

<!-- async load function -->
<script>
    function async(u, c) {
      var d = document, t = 'script',
          o = d.createElement(t),
          s = d.getElementsByTagName(t)[0];
      o.src = u;
      if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
      s.parentNode.insertBefore(o, s);
    }
</script>

<!-- anchor-js, Doc:http://bryanbraun.github.io/anchorjs/ -->

<style>
    /* place left on bigger screen */
    @media all and (min-width: 800px) {
        .anchorjs-link{
            position: absolute;
            left: -0.75em;
            font-size: 1.1em;
            margin-top : -0.1em;
        }
    }
</style>


    <!-- Footer -->
<footer>
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <ul class="list-inline text-center">
                    <!-- kill the Facebook and Weibo -->
                    <li>
                        <a href="/feed.xml">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-rss fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                    

                    
                    
                    <!--
                    
                    -->

                    <!--
                    
                    -->

                    
                    <!--
                    
                    -->

                </ul>
                <p class="copyright text-muted">
                &copy; 2016  Chengcen  ❖ Powered by Jekyll.
                </p>
            </div>
        </div>
    </div>
</footer>

<!-- jQuery -->
<script src="/js/jquery.min.js "></script>

<!-- Bootstrap Core JavaScript -->
<script src="/js/bootstrap.min.js "></script>

<!-- Custom Theme JavaScript -->
<script src="/js/clean-blog.min.js "></script>



<!-- Highlight.js -->
<script>
    async("http://cdn.bootcss.com/highlight.js/8.6/highlight.min.js",function(){
        hljs.initHighlightingOnLoad();
    })
</script>
<link href="http://cdn.bootcss.com/highlight.js/8.6/styles/github.min.css" rel="stylesheet">


</body>

</html>