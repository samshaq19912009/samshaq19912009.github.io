---
layout: post
title: EM Algorithm
date: 2016-04-01
categories: blog
tags: [Machine Learning,Notes]
description: EM
---






# Mixtures of Gaussians and the EM algorithm

Given a training set $\{x^{(1)},....x^{(m)} \}$

Model the data by specifying a joint distribution $p(x^{(i)}, z^{(i)}) = p(x^{(i)} z^{(i)}) p(z^{(i)}) $, Here $z^{(i)} \sim\mbox{Multinomail}(\phi)$ and the parameter $\phi_j$  
gives $p(z^i) = j$ and $x^i | z^i = j \sim N(\mu_j,\Sigma_j)$ 

The likelihood of our data
$$ l(\phi, \mu, \Sigma) = \sum_{i=1}^m log p(x^{(i)}; \phi, \mu, \Sigma)  \\
	= \sum_{i=1}^m log \sum_{z^{(i)}=1}^{k} p(x^{(i)}|z^{(i)}; \mu, \Sigma) p(z^i; \phi)$$
	
Question: unknown z

**Expectation-Maximization Algorithm**

Repeat unitil convergence:

(E-step) For each i,j set
	$$w^{(i)}_j = p(z^{(i)}=j  |  x^{(i)}; \phi,\mu,\Sigma)$$
	
(M-step) Update the parameters:
$$ \phi_j = \frac{1}{m} \sum_{i=1}^m w^{(i)}_j$$
$$ \mu_j = \frac{\sum_{i=1}^m w^{(i)}_j x^{(i)}}{\sum_{i=1}^m w^{(i)}_j}$$
$$ \Sigma_j = \frac{\sum_{i=1}^m w^{(i)}_j (x^{(i)}-\mu_j)(x^{(i)}-\mu_j)^T}{\sum_{i=1}^m w^{(i)}_j}$$

### Jensen's inequality
**Theorem** Let f be a convex function, and let X be random variable, then:
$$ E[f(X)] \geq f(E[X])$$

Moreover, if f is strictly convex, then the equality sign only holds for X is a constant


### EM algorithm induction

For each i, let $Q_i$ be some distribution over z's. ($\sum_z Q_i(z) = 1$, $Q_i(z) \geq 0$)
$$ \sum_i \log p(x^{(i)};\theta) = \sum_i \log \sum_{z^{(i)}} p(x^{(i)},z^{(i)};\theta) 
$$
$$ = \sum_i \log \sum_{z^{(i)}} Q_i(z^{(i)}) \frac{p(x^{(i)},z^{(i)};\theta) }{Q_i(z^{(i)})}
$$ 
$$ \geq \sum_i \sum_{z^{(i)}} Q_i(z^{(i)}) \log \frac{p(x^{(i)},z^{(i)};\theta) }{Q_i(z^{(i)})}
$$

Notice that $\log(x)$ is a concave function and here $f(x) = \frac{p(x^{(i)},z^{(i)};\theta) }{Q_i(z^{(i)})} $

Now for a particular $\theta$, try to make the lower-bound tight, requiring that 
$$  \frac{p(x^{(i)},z^{(i)};\theta) }{Q_i(z^{(i)})} = c$$

This can be done by choosing $ Q_i(z^{(i)}) \propto p(x^{(i)},z^{(i)};\theta)  $

Moreover, since $\sum_z Q_i(z) = 1$, we know that
$$ Q_i(z^{(i)}) = \frac{p(x^{(i)},z^{(i)};\theta)}{\sum_z p(x^{(i)},z^{(i)};\theta)} = p(z^{(i)}|x^{(i)};\theta)
$$

Thus, simply setting the $Q_i$ to be the posterior distribtuion of $z^{(i)}$

How do we know the algorithm will converge?

$$ l(\theta^{(t)}) = \sum_i \sum_{z^{(i)}} Q_i^t(z^{(i)}) \log \frac{p(x^{(i)},z^{(i)};\theta^{(t)}) }{Q_i(z^{(i)})}
$$

The parameter $\theta^{(t+1)}$ are then obtained by maximizing the right hand side of the equation above, thus,
$$ l(\theta^{(t+1)}) \geq \sum_i \sum_{z^{(i)}} Q_i^t(z^{(i)}) \log \frac{p(x^{(i)},z^{(i)};\theta^{(t+1)}) }{Q_i(z^{(i)})}$$
$$= \sum_i \sum_{z^{(i)}} Q_i^t(z^{(i)}) \log \frac{p(x^{(i)},z^{(i)};\theta^{(t)}) }{Q_i(z^{(i)})} = l(\theta^{(t)} 
$$

**Remark** If we define:
$$ J(Q,\theta) = \sum_i \sum_{z^{(i)}} Q_i(z^{(i)}) \log \frac{p(x^{(i)},z^{(i)};\theta) }{Q_i(z^{(i)})} $$
The EM can also be viewed as a coordinate ascent on J, in which the E-step maximizes it with respect to Q. and the M-step maximizes it with respect to $\theta$.

