---
layout: post
title: Factor Analysis
date: 2016-03-23
categories: blog
tags: [Machine Learning,Notes]
description: Factor Analysis
---
# Factor Analysis

Consider the setting the dimension of the data n is larger than the training size m, if we still use the maximum likelihood estimators,

$$ \mu = \frac{1}{m}\sum_{i=1}^m x^{(i)}$$

$$ \Sigma = \frac{1}{m}\sum_{i=1}^m (x^{(i)} - \mu) (x^{(i)} - \mu)^T $$

We would find the matrix $\Sigma$ is **singular**

## Restriction of covariance matrix

Fit a covariance matrix $\Sigma$ that is diagonal.
$$ \Sigma_{jj} = \frac{1}{m}\sum_{i=1}^m (x^{(i)}_j - \mu_j)^2  $$

Another restriction on the covariance matrix : its diagonal entries must be equal. We have $\Sigma = \sigma^2 I$
$$ \sigma^2 = \frac{1}{mn}\sum_{i=1}^m (x^{(i)}_j - \mu_j)^2  $$

If we were fitting a full, unconstrained, covariance matrix $\Sigma$ to data, it was necessary that $m \geq n+1$, now with these two constraints, we only need $m \geq 2$




## Marginals and conditions of Gaussians
Suppose we have a vector-valued random variables:

$$x= \begin{pmatrix}
x_1\\
x_2
\end{pmatrix} $$
whre $x_1 \in \mathbb{R}^r$, $x_2 \in \mathbb{R}^s$, and $x \in \mathbb{R}^{r+s}$. Suppose $x \sim N(\mu, \Sigma)$, where 
$$\mu= \begin{pmatrix}
\mu_1\\
\mu_2
\end{pmatrix}, \Sigma = \begin{pmatrix}
\Sigma_{11} & \Sigma_{12}\\
\Sigma_{21} & \Sigma_{22}
\end{pmatrix} $$
And the covariance matrix of x should be 
$$Cov(x) = E\begin{pmatrix}
(x_1 - \mu) (x_1- \mu)^T & (x_1 - \mu) (x_2- \mu)^T\\
(x_2 - \mu) (x_1- \mu)^T & (x_2 - \mu) (x_2- \mu)^T
\end{pmatrix} 
$$

The conditional distributon of $x_1$ given $x_2 $ is $N(\mu_{1,2}, \Sigma_{1,2})$
where
$$ \mu_{1,2} = \mu_1 + \Sigma_{12}\Sigma_{22}^{-1}(x_2 - \mu_2) $$
$$ \Sigma_{1,2} = \Sigma_{11} - \Sigma_{12} \Sigma_{22}^{-1} \Sigma_{21} $$


## The factor analysis model
In the factor analysis model, we posit a joint distribution on (x,z) as follows, where $z \in \mathbb{R}^k$ is a latent random variable
$$z \sim N(0,I)$$
$$(x,z) \sim N(\mu + \Lambda z, \Psi) $$
Thus, we can image that each datapoint $x^{(i)}$ is generated by sampling a k dimension multivariate Gaussian $z^{(i)}$. Then it is mapped to a k-dimensional affine space of $\mathbb{R}^n$ by computing $\mu + \Lambda z^{(i)}$.

Equivalently, we can therefore also define the factor analysis model according to
$$z \sim N(0,I)$$
$$\epsilon \sim N(0,\Psi)  $$
$$x = \mu + \Lambda z + \epsilon $$

Now the variable z and x have a joint Gaussian distribution
$$ \begin{pmatrix}
z\\
x
\end{pmatrix}  \sim N(\mu_{zx}, \Sigma)
$$

We know that $E[z] = 0$, also we have
$$E[x] = E[\mu + \Lambda z + \epsilon] = \mu
$$

Now since $z ~ N(O,I)$, we can easily find that $\Sigma_{zz} = Cov(z) = I$, Also
$$ E[(z- E[z]) (x- E[x])^T] = E[z(\Lambda z + \epsilon)^T ] = \Lambda^T$$

Similarly, we can find $\Sigma_{xx}$ as follows:
$$ E[(x- E[x]) (x- E[x])^T] = \Lambda \Lambda^T + \Psi  $$

Putting everything together, we therefore have that 
$$ \begin{pmatrix}
z\\
x
\end{pmatrix}  \sim N(\begin{pmatrix}
0\\
\mu
\end{pmatrix}, \begin{pmatrix}
I & \Lambda^T\\
\Lambda^T & \Lambda \Lambda^T + \Psi  
\end{pmatrix})
$$
Hence, we also see that the marginal distribution of x is give by $x \sim N(\mu, \Lambda \Lambda^T + \Psi )$. Thus given a training set, we can write the log likelihood of the parameters:

$$l(\mu,\Lambda,\Psi) = log \prod_{i=1}^m \frac{1}{(2\pi)^{n/2}(\Lambda \Lambda^T + \Psi)^{1/2}}\exp(-\frac{1}{2}((x^{(i)}- \mu)^T (\Lambda \Lambda^T + \Psi)^{-1}(x^{(i)}- \mu)) $$ 

## EM for factor analysis

The derivation for the E-step is easy, we need to compute $Q_i(z^{(i)}) = p(z^{(i)}:x^{(i)};\mu, \Lambda, \Psi)$, using the conditional distribution of a Gaussian, we have that


$$\mu_{z^{(i)} : x^{(i)}} = \Lambda^T (\Lambda \Lambda^T + \Psi)^{-1} (x^{(i)}- \mu)$$


$$\Sigma_{z^{(i)} : x^{(i)}} = I - \Lambda^T (\Lambda \Lambda^T + \Psi)^{-1} \Lambda$$

So 

$$Q_i(z^{(i)}) = \frac{1}{(2\pi)^{k/2}(\Sigma_{z^{(i)} : x^{(i)}})^{1/2}} \exp(-\frac{1}{2}(z^{(i)}- \mu_{z^{(i)} : x^{(i)}})^T (\Sigma_{z^{(i)} : x^{(i)}})^{-1}(z^{(i)}- \mu_{z^{(i)} : x^{(i)}})
$$


Now for the M-step, here we need to maximize

$$\sum_{i=1}^m \int_{z^{(i)}}Q_i(z^{(i)})\log \frac{p(x^{(i)},z^{(i)};\mu, \Lambda, \Psi)}{Q_i(z^{(i)})}d z^{(i)}
$$

First we try to maximize with respect to $\Lambda$. 

We can simply this equation as an expectaction value as follows

$$\sum_{i=1}^m E_{z^{(i)} \sim Q_{i}} [\log p(x^{(i)},z^{(i)};\mu, \Lambda, \Psi) + \log p(z^{(i)}) - \log Q_i(z^{(i)})] 
$$

Dropping out terms that do not depend on the parameters,

$$\sum_{i=1}^m E[\log p(x^{(i)}:z^{(i)};\mu, \Lambda, \Psi)]$$

$$= \sum_{i=1}^m E[\log \frac{1}{(2\pi)^{n/2}(\Psi)^{1/2}}\exp(-\frac{1}{2}((x^{(i)}- \mu - \Lambda z^{(i)})^T (\Psi)^{-1}(x^{(i)}- \mu) - \Lambda z^{(i)}))]
$$


$$= \sum_{i=1}^m E[-\frac{1}{2} \log \Psi -\frac{n}{2} \log (2\pi) - \frac{1}{2}((x^{(i)}- \mu - \Lambda z^{(i)})^T (\Psi)^{-1}(x^{(i)}- \mu) - \Lambda z^{(i)})]
$$
