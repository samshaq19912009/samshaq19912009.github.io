---
layout: post
title: Apriori algorithm
date: 2016-06-15
categories: blog
tags: [Apriori,Machine Learning]
description: Apriori
---



# Apriori algorithm

## Assoication analysis

Pros: Easy to code up

Cons: May be slow on large datasets

Works with: Numeric values, nominal values


**Support** the percentage of the dataset that contains this itemset

**Confidence**: an association rule like A -> B, the confidence of this rule is defined as  support(A)/support(B)

## Apriori principle

1. Collect: Any method
2. Prepare: Any data type would work 
3. Analyze: Any method
4. Train: Use the Apriori algorithm to find the frequent item
5. Test: does not apply
6. Use: find frequent itemsets and association rules


**Apriori principle**:

The Apriori principle says that if an itemset is frequent, then all of its subsets are frequent. Reversely, if an itemset is not frequent, then any bigger itemset contains it would not be frequent

## Finding frequent itemsets with the Apriori algorithm

### Generating candidate itemsets

Ck : all the possible combination contains k elements

Lk : returned itemset with k+1 elements



C1 : candidate set 1, contains one item from the dataset

L0: successful candidate with one item


C2 : generated by L0, possible combinations with two items

L1 : successful candidation with two items

C3: formed by L1

.....

```Python

def loadDataSet():
    return [[1, 3, 4], [2, 3, 5], [1, 2, 3, 5], [2, 5]]

def createC1(dataSet):
    C1 = []
    for transaction in dataSet:
        for item in transaction:
            if not [item] in C1:
                C1.append([item])
                
    C1.sort()
    return map(frozenset, C1)#use frozen set so we
                            #can use it as a key in a dict    
                            
##load the data first
data = loadDataSet()
D = map(set, data)

C1 = createC1(data)

```

The code to generate Lk


```Python
def scanD(D, Ck, minSupport):
    ssCnt = {}
    for tid in D:
        for can in Ck:
            if can.issubset(tid):
                if not ssCnt.has_key(can): ssCnt[can]=1
                else: ssCnt[can] += 1
                    
    numItems = float(len(D))
    retList = []
    supportData = {}
    for key in ssCnt:
        support = ssCnt[key]/numItems
        if support >= minSupport:
            retList.insert(0,key)
        supportData[key] = support
    return retList, supportData

```

For each Lk generate Ck+1

the full algorithm is shown below

```Python
def aprioriGen(Lk, k): #creates Ck
    retList = []
    lenLk = len(Lk)
    for i in range(lenLk):
        for j in range(i+1, lenLk): 
            L1 = list(Lk[i])[:k-2]; L2 = list(Lk[j])[:k-2]
            L1.sort(); L2.sort()
            if L1==L2: #if first k-2 elements are equal
                retList.append(Lk[i] | Lk[j]) #set union
    return retList

def apriori(dataSet, minSupport = 0.5):
    C1 = createC1(dataSet)
    D = map(set, dataSet)
    L1, supportData = scanD(D, C1, minSupport)
    L = [L1]
    k = 2
    while (len(L[k-2]) > 0):
        Ck = aprioriGen(L[k-2], k)
        Lk, supK = scanD(D, Ck, minSupport)#scan DB to get Lk
        supportData.update(supK)
        L.append(Lk)
        k += 1
    return L, supportData

```

## Mining association rules from frequent item sets

**levelwise** approach



### Function 1: calculate confidence--start with size 2 itemset

Give the total set freqSet,

try every possible itemset in H

compute the confidence


```Python

def calcConf(freqSet, H, supportData, brl, minConf=0.7):
    prunedH = [] #create new list to return
    for conseq in H:
        conf = supportData[freqSet]/supportData[freqSet-conseq] #calc confidence
        if conf >= minConf: 
            print freqSet-conseq,'-->',conseq,'conf:',conf
            brl.append((freqSet-conseq, conseq, conf))
            prunedH.append(conseq)
    return prunedH

```


### Function 2: Try further merge into larget item list

```Python
def rulesFromConseq(freqSet, H, supportData, brl, minConf=0.7):
    m = len(H[0]) # m is the size of the input itemset
    # try find m+1 itemset 
    
    if (len(freqSet) > (m + 1)): #try further merging
        Hmp1 = aprioriGen(H, m+1)#create Hm+1 new candidates
        Hmp1 = calcConf(freqSet, Hmp1, supportData, brl, minConf)
        if (len(Hmp1) > 1):    #need at least two sets to merge
            rulesFromConseq(freqSet, Hmp1, supportData, brl, minConf)
```

### Function 3: Combine all these two, return the big rule list


```Python
def generateRules(L, supportData, minConf=0.7):  #supportData is a dict coming from scanD
    bigRuleList = []
    for i in range(1, len(L)):#only get the sets with two or more items
        for freqSet in L[i]:
            H1 = [frozenset([item]) for item in freqSet]
            if (i > 1):
                rulesFromConseq(freqSet, H1, supportData, bigRuleList, minConf)
            else:
                calcConf(freqSet, H1, supportData, bigRuleList, minConf)
    return bigRuleList         




```
