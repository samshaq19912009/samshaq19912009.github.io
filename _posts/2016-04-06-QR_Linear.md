---
layout: post
title: Cholesky and QR algorithm
date: 2016-04-06
categories: blog
tags: [Machine Learning,Notes]
description: Chokesky and QR
---



In linear regression we try to minimize the train error:
$$\min (X \beta - y)^2
$$
and solve the normal equations :
$$X^T X \theta = X^T y
$$
The solution is obtained by 
$$ \theta = (X^T X)^{-1} X^T y
$$

However, direct calculations are very hard.

## Method 1 : Cholesky factorization

* Calcluate $C = X^T X$ $ \quad \frac{1}{2}n(n+1)(2m-1)$ flops
* Cholesky factorization $C = LL^T$ ($1/3n^3$) flops
* calculate $d = X^T y$ $2mn$ flops
* solve $Lz =d  $ by forward substitutions $n^2$ flops
* Solve $L^T \beta = z$ by backward substitutions $n^2$ flops

**Cost for large m,n**
$$mn^2 + (1/3) n^3$$ 

## Method 2 : QR Algorithm
if X is $m \times n$ and left-invertible then it can be factored as
$$A = QR$$
* R is $n \times n$ and upper triangular with $r_{ii} > 0$
* Q is $m \times n$ and orthogonal $Q^TQ = I$

Using this to solve the previous normal equations
$$ X= QR $$
$$ X^TX\beta= X^Ty $$
$$ R^TR\beta= R^TQ^Ty $$
$$ R\beta = Q^Ty $$

Analysis of Algorithm

* QR factorization of X : $X = QR$ ($2mn^2$) flops
* form $d = Q^T$ ($2mn$) flops 
* solve $R\beta = d$ by back substitution ($n^2$) flops

**Cost for large m,n**
$$2mn^2$$ 

### Summary

#### Cost for dense X
* Method 1 is always faster (twice as fast if $ m \gg n $)

#### Cost for large sparse X

* Can perform a faster sparse Cholesky
factorization
* method 2: exploiting sparsity in QR factorization is more difficult

#### numerical stability: 
method 2 is more accurate

#### in practice: 
method 2 is preferred; method 1 is often used when A is very
large and sparse
