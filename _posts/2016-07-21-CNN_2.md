---
layout: post
title: CNN 读书笔记（2）
date: 2016-07-17
categories: blog
tags: [Machine Learning,Notes, CNN, SVM]
description: CNN

---

# Linear Classification


Two disadvantages for KNN classifier:

* remember all the training data and store it for future comparisons

* Classify new test sample is expensive


## Parameterized mapping from images to label scores

**Score functions** Maps the raw data to class scores

Assume traning dataset of images $x_i \in R^D$, each associated with a label $y_i$, here $ i = 1 \dots N$ and $y_i \in { 1 \dots K }$. That is, we have N samples and K distance label features. The **score function** $f: R^D \mapsto R^K$ that maps the raw image pixels to class scores.

**Linear functions** 
$$f(x_i, W, b) =  W x_i + b$$

**Interpreting a linear classifier**

![](http://cs231n.github.io/assets/imagemap.jpg)

**Bias trick**

$f(x_i, W, b) =  W x_i + b$  

Adding extra dimensions:

$f(x_i, W) =  W x_i$

![](http://cs231n.github.io/assets/wb.jpeg)



**Loss functions**

Quantifies the agreement between the predicted scores and the ground truth labels

## Loss Function

### Multiclass Support Vector Machine Loss

$L_i = \sum_{j\neq y_i} \max(0, s_j - s_{y_i} + \Delta)
$

In summary, the SVM loss function wants the score of the correct class $y_i$ to be larger than the incorrect class scores by at least by Δ (delta). If this is not the case, we will accumulate loss.

> The loss function quantifies our unhappiness with predictions on the training set

In this linear setting

$L_i = \sum_{j\neq y_i} \max(0, w_j^T x_i - w_{y_i}^T x_i + \Delta)$

![](http://cs231n.github.io/assets/margin.jpg)


#### Regularization

$ R(W) = \sum_k\sum_l W_{k,l}^2$

#### Total loss functions

$L = \frac{1}{N} \sum_i \sum_{j\neq y_i} \left[ \max(0, f(x_i; W)_j - f(x_i; W)_{y_i} + \Delta) \right] + \lambda \sum_k\sum_l W_{k,l}^2$


## Code implementation

### SVM naive 

```Python
def svm_loss_naive(W, X, y, reg):
  """
  Structured SVM loss function, naive implementation (with loops).

  Inputs have dimension D, there are C classes, and we operate on minibatches
  of N examples.

  Inputs:
  - W: A numpy array of shape (D, C) containing weights.
  - X: A numpy array of shape (N, D) containing a minibatch of data.
  - y: A numpy array of shape (N,) containing training labels; y[i] = c means
    that X[i] has label c, where 0 <= c < C.
  - reg: (float) regularization strength

  Returns a tuple of:
  - loss as single float
  - gradient with respect to weights W; an array of same shape as W
  """
  dW = np.zeros(W.shape) # initialize the gradient as zero

  # compute the loss and the gradient
  num_classes = W.shape[1]
  num_train = X.shape[0]
  loss = 0.0
  for i in xrange(num_train):
    scores = X[i].dot(W) ## C*1 score

    #dS = np.sum(X[i])
    correct_class_score = scores[y[i]]
    diff_count = 0
    for j in xrange(num_classes):

        if j == y[i]:
            continue
        margin = scores[j] - correct_class_score + 1 # note delta = 1
        if margin > 0:
            diff_count += 1 #count for all the incorrect
            dW[:,j] += X[i] #gradient update for incorrect rows
            loss += margin
    dW[:,y[i]] += -diff_count* X[i]#gradient update for correct rows

  # Right now the loss is a sum over all training examples, but we want it
  # to be an average instead so we divide by num_train.
  loss /= num_train
  dW /= num_train
  # Add regularization to the loss.
  loss += 0.5 * reg * np.sum(W * W)
  dW += reg*W

  return loss, dW


```


### SVM Vectorized

```Python

def svm_loss_vectorized(W, X, y, reg):
  """
  Structured SVM loss function, vectorized implementation.

  Inputs and outputs are the same as svm_loss_naive.
  """
  loss = 0.0
  dW = np.zeros(W.shape) # initialize the gradient as zero

  num_train = X.shape[0]

  scores = X.dot(W)
  correct_class_score = scores[np.arange(num_train), y]
  margins = np.maximum(0, scores - correct_class_score[:, np.newaxis] + 1)
  margins[np.arange(num_train), y] = 0
  loss = np.sum(margins)
  loss /= num_train
  loss += 0.5 * reg * reg * np.sum(W*W)


  X_mask = np.zeros(margins.shape)
  X_mask[margins > 0] = 1
  incorrect_counts = np.sum(X_mask, axis=1)
  X_mask[np.arange(num_train),y] = -incorrect_counts
  dW = X.T.dot(X_mask)

  dW /= num_train
  dW += reg * W


  return loss, dW
```


## Softmax classifer

The Softmax classifier gives a slightly more intuitive output: **normalized class probabilities**

$L_i = -\log\left(\frac{e^{f_{y_i}}}{ \sum_j e^{f_j} }\right) \hspace{0.5in} \text{or equivalently} \hspace{0.5in} L_i = -f_{y_i} + \log\sum_j e^{f_j}$

#### Information theory view####

The cross-entropy between a “true” distribution p and an estimated distribution q is defined as:

$H(p,q) = - \sum_x p(x) \log q(x)$

The Softmax classifier is hence minimizing the cross-entropy between the estimated class probabilities $q = e^{f_{y_i}}  / \sum_j e^{f_j}$as seen above) and the “true” distribution, which in this interpretation is the distribution where all probability mass is on the correct class ($p = [0, \ldots 1, \ldots, 0]$ contains a single 1 at the yi y i-th position.)

#### Softmax naive ####

```Python
def softmax_loss_naive(W, X, y, reg):
    """
  Softmax loss function, naive implementation (with loops)

  Inputs have dimension D, there are C classes, and we operate on minibatches
  of N examples.

  Inputs:
  - W: A numpy array of shape (D, C) containing weights.
  - X: A numpy array of shape (N, D) containing a minibatch of data.
  - y: A numpy array of shape (N,) containing training labels; y[i] = c means
    that X[i] has label c, where 0 <= c < C.
  - reg: (float) regularization strength

  Returns a tuple of:
  - loss as single float
  - gradient with respect to weights W; an array of same shape as W
  """
    # Initialize the loss and gradient to zero.
    loss = 0.0
    dW = np.zeros(W.shape)
    # return loss, dW
    # dW = np.zeros(W.shape) # initialize the gradient as zero

    # compute the loss and the gradient
    num_classes = W.shape[1]
    num_train = X.shape[0]
    loss = 0.0
    for i in xrange(num_train):
        scores = X[i].dot(W)  ## C*1 score
        scores -= np.max(scores)

        
        correct_class_score = scores[y[i]]
        
        sum_j = np.sum(np.exp(scores))

        n = np.exp(correct_class_score)
        loss -= np.log(n / sum_j)

        for j in xrange(num_classes):
          if j == y[i]:
            dW[:,j] -= X[i]
          pj = np.exp(scores[j]) / sum_j
          dW[:,j] += pj * X[i]

    loss /= num_train
    dW /= num_train

    # Add regularization to the loss.
    loss += 0.5 * reg * np.sum(W * W)

    dW += reg * W
    return loss, dW

```

#### Softmax Vectorized ####

```Python

def softmax_loss_vectorized(W, X, y, reg):
    """
  Softmax loss function, vectorized version.

  Inputs and outputs are the same as softmax_loss_naive.
  """
    # Initialize the loss and gradient to zero.

    num_train = X.shape[0]

    dW = np.zeros_like(W)
    scores = X.dot(W)
    scores -= np.max(scores,axis=1)[:,np.newaxis]


    correct_class_score = scores[np.arange(num_train), y]
    divide = np.sum(np.exp(scores),axis=1)
    n = np.exp(correct_class_score)
    error = -1 * np.log( n / divide)
    loss = np.sum(error)


    X_mask = np.exp(scores) / divide[:,np.newaxis]

    #X_mask = np.zeros(error.shape)
    X_mask[np.arange(num_train),y] -= 1

    dW = X.T.dot(X_mask)

    loss /= num_train
    dW /= num_train

    loss += 0.5 * reg * np.sum(W * W)
    dW += reg * W

    return loss, dW
```

### Stochastic gradient Decent

```Python
class LinearClassifier(object):

  def __init__(self):
    self.W = None

  def train(self, X, y, learning_rate=1e-3, reg=1e-5, num_iters=100,
            batch_size=200, verbose=False):
    """
    Train this linear classifier using stochastic gradient descent.

    Inputs:
    - X: A numpy array of shape (N, D) containing training data; there are N
      training samples each of dimension D.
    - y: A numpy array of shape (N,) containing training labels; y[i] = c
      means that X[i] has label 0 <= c < C for C classes.
    - learning_rate: (float) learning rate for optimization.
    - reg: (float) regularization strength.
    - num_iters: (integer) number of steps to take when optimizing
    - batch_size: (integer) number of training examples to use at each step.
    - verbose: (boolean) If true, print progress during optimization.

    Outputs:
    A list containing the value of the loss function at each training iteration.
    """
    num_train, dim = X.shape[0], X.shape[1]
    num_classes = np.max(y) + 1 # assume y takes values 0...K-1 where K is number of classes
    if self.W is None:
      # lazily initialize W
      self.W = 0.001 * np.random.randn(dim, num_classes)

    # Run stochastic gradient descent to optimize W
    loss_history = []
    for it in xrange(num_iters):
      X_batch = None
      y_batch = None



      

      # evaluate loss and gradient
      loss, grad = self.loss(X, y, reg)
      loss_history.append(loss)

      self.W +=  - grad * learning_rate


      # perform parameter update


      if verbose and it % 100 == 0:
        print 'iteration %d / %d: loss %f' % (it, num_iters, loss)

    return loss_history

  def predict(self, X):
    """
    Use the trained weights of this linear classifier to predict labels for
    data points.

    Inputs:
    - X: D x N array of training data. Each column is a D-dimensional point.

    Returns:
    - y_pred: Predicted labels for the data in X. y_pred is a 1-dimensional
      array of length N, and each element is an integer giving the predicted
      class.
    """
    y_pred = np.zeros(X.shape[1])

    score = X.dot(self.W)

    y_pred = np.argmax(score,axis=1)

   
    return y_pred

  def loss(self, X_batch, y_batch, reg):
    """
    Compute the loss function and its derivative.
    Subclasses will override this.

    Inputs:
    - X_batch: A numpy array of shape (N, D) containing a minibatch of N
      data points; each point has dimension D.
    - y_batch: A numpy array of shape (N,) containing labels for the minibatch.
    - reg: (float) regularization strength.

    Returns: A tuple containing:
    - loss as a single float
    - gradient with respect to self.W; an array of the same shape as W
    """
    pass


```