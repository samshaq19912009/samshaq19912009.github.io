---
layout: post
title: CNN 读书笔记（3） Netural Network 初入门
date: 2016-07-17
categories: blog
tags: [Machine Learning,Notes, CNN]
description: CNN

---

### Different Activation Function

![Two common activation function](http://cs231n.github.io/assets/nn1/sigmoid.jpeg)![](http://cs231n.github.io/assets/nn1/tanh.jpeg)

#### Drawback for sigmod function

**Sigmoids saturate and kill gradients**

**Sigmoid outputs are not zero-centered.**


**Tanh function** is zero-centered


**Why Relu**

1. Converge faster

2. Simply calculation

3. Unfortunately, ReLU units can be fragile during training and can “die”


### Neural Network architectures


![](http://cs231n.github.io/assets/nn1/neural_net.jpeg)

### Representational power

Neural Networks with at least one hidden layer are universal approximators :

given any continuous function f(x) and some ϵ>0 , there exists a Neural Network g(x) with one hidden layer (with a reasonable choice of non-linearity, e.g. sigmoid) such that ∀x,∣f(x)−g(x)∣<ϵ. In other words, the **neural network can approximate any continuous function**

#### Setting the number of layers in sizes

![](http://cs231n.github.io/assets/nn1/layer_sizes.jpeg)

#### Controlling the complexity 

![](http://cs231n.github.io/assets/nn1/reg_strengths.jpeg)

